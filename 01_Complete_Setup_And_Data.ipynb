{"cells":[{"cell_type":"code","source":["# COMPLETE SETUP & DATA GENERATION - RUN THIS FIRST\n","# Make sure Lakehouse is attached before running!\n","\n","print(\"üöÄ REIMAGE-AI SMART PARKING - COMPLETE SETUP\")\n","\n","# =============================================================================\n","# STEP 1: Verify Lakehouse Connection\n","# =============================================================================\n","print(\"üîß STEP 1: VERIFYING LAKEHOUSE CONNECTION...\")\n","\n","try:\n","    tables = spark.sql(\"SHOW TABLES\").collect()\n","    print(f\"‚úÖ Lakehouse connected! Found {len(tables)} tables\")\n","except Exception as e:\n","    print(\"‚ùå No Lakehouse attached! Please manually attach Lakehouse first:\")\n","    print(\"1. Click 'Add' in top-right of notebook\")\n","    print(\"2. Select 'Existing Lakehouse'\")\n","    print(\"3. Choose 'ParkingDataLakehouse'\")\n","    print(\"4. Run this cell again\")\n","    raise e\n","\n","# =============================================================================\n","# STEP 2: Clean Up Existing Tables (If Any)\n","# =============================================================================\n","print(\"\\nüßπ STEP 2: CLEANING UP EXISTING TABLES...\")\n","\n","tables_to_clean = [\"ParkingSensorData\", \"TrafficCameraData\", \"HistoricalTraffic\", \"SensorMaintenanceData\"]\n","\n","for table_name in tables_to_clean:\n","    try:\n","        spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n","        print(f\"‚úÖ Dropped table: {table_name}\")\n","    except Exception as e:\n","        print(f\"‚ÑπÔ∏è  Could not drop {table_name}: {e}\")\n","\n","# =============================================================================\n","# STEP 3: Create Fresh Tables\n","# =============================================================================\n","print(\"\\nüìã STEP 3: CREATING FRESH TABLES...\")\n","\n","tables_sql = {\n","    \"ParkingSensorData\": \"\"\"\n","        CREATE TABLE ParkingSensorData (\n","            sensor_id STRING,\n","            timestamp TIMESTAMP,\n","            occupancy_status BOOLEAN,\n","            vehicle_count INT,\n","            temperature DOUBLE,\n","            vibration_level DOUBLE,\n","            location STRING,\n","            parking_zone STRING,\n","            transaction_hash STRING\n","        ) USING DELTA\n","    \"\"\",\n","    \"TrafficCameraData\": \"\"\"\n","        CREATE TABLE TrafficCameraData (\n","            camera_id STRING,\n","            timestamp TIMESTAMP,\n","            image_url STRING,\n","            vehicle_count INT,\n","            traffic_density DOUBLE,\n","            average_speed INT,\n","            congestion_level STRING,\n","            processed BOOLEAN,\n","            yolo_results STRING\n","        ) USING DELTA\n","    \"\"\",\n","    \"HistoricalTraffic\": \"\"\"\n","        CREATE TABLE HistoricalTraffic (\n","            zone_id STRING,\n","            date DATE,\n","            hour INT,\n","            average_occupancy DOUBLE,\n","            traffic_volume INT,\n","            weather_condition STRING,\n","            event_day BOOLEAN\n","        ) USING DELTA\n","    \"\"\"\n","}\n","\n","for table_name, sql in tables_sql.items():\n","    try:\n","        spark.sql(sql)\n","        print(f\"‚úÖ Created: {table_name}\")\n","    except Exception as e:\n","        print(f\"‚ùå Failed to create {table_name}: {e}\")\n","\n","print(\"üéâ Tables created successfully!\")\n","\n","# =============================================================================\n","# STEP 4: Generate Synthetic Data\n","# =============================================================================\n","print(\"\\nüöÄ STEP 4: GENERATING SYNTHETIC DATA...\")\n","\n","from faker import Faker\n","from datetime import datetime, timedelta\n","import random\n","import json\n","\n","class DataGenerator:\n","    def __init__(self):\n","        self.fake = Faker()\n","    \n","    def generate_parking_data(self, num_records=200):\n","        print(f\"üÖøÔ∏è Generating {num_records} parking records...\")\n","        records = []\n","        locations = ['Downtown', 'Shopping Mall', 'Airport', 'Hospital', 'University']\n","        zones = ['ZONE_A', 'ZONE_B', 'ZONE_C', 'ZONE_D', 'ZONE_E']\n","        \n","        for i in range(num_records):\n","            sensor_id = f\"SENSOR_{random.randint(1, 50):03d}\"\n","            timestamp = self.fake.date_time_between(start_date=\"-30d\", end_date=\"now\")\n","            \n","            # Realistic occupancy patterns\n","            hour = timestamp.hour\n","            if 8 <= hour <= 18:  # Business hours\n","                occupancy_prob = 0.7\n","            else:  # Off hours\n","                occupancy_prob = 0.3\n","                \n","            occupied = random.random() < occupancy_prob\n","            \n","            records.append({\n","                'sensor_id': sensor_id,\n","                'timestamp': timestamp,\n","                'occupancy_status': bool(occupied),\n","                'vehicle_count': int(1 if occupied else 0),  # Explicit int conversion\n","                'temperature': float(round(random.uniform(15.0, 35.0), 2)),\n","                'vibration_level': float(round(random.uniform(0.1, 5.0), 2)),\n","                'location': str(random.choice(locations)),\n","                'parking_zone': str(random.choice(zones)),\n","                'transaction_hash': str(self.fake.sha256())\n","            })\n","        \n","        print(f\"‚úÖ Generated {len(records)} parking records\")\n","        return records\n","    \n","    def generate_traffic_data(self, num_records=100):\n","        print(f\"üö¶ Generating {num_records} traffic records...\")\n","        records = []\n","        \n","        for i in range(num_records):\n","            camera_id = f\"CAM_{random.randint(1, 20):03d}\"\n","            timestamp = self.fake.date_time_between(start_date=\"-30d\", end_date=\"now\")\n","            \n","            # Time-based traffic patterns\n","            hour = timestamp.hour\n","            if 7 <= hour <= 9 or 16 <= hour <= 18:  # Rush hours\n","                vehicles = random.randint(20, 50)\n","                density = round(random.uniform(0.7, 0.95), 2)\n","                speed = random.randint(20, 40)\n","            else:  # Normal hours\n","                vehicles = random.randint(5, 25)\n","                density = round(random.uniform(0.2, 0.6), 2)\n","                speed = random.randint(40, 60)\n","                \n","            congestion = \"HIGH\" if density > 0.7 else \"MEDIUM\" if density > 0.4 else \"LOW\"\n","            \n","            # YOLO simulation data\n","            yolo_data = {\n","                'vehicles_detected': vehicles,\n","                'confidence': round(random.uniform(0.85, 0.98), 2),\n","                'processing_time_ms': random.randint(100, 300)\n","            }\n","            \n","            records.append({\n","                'camera_id': str(camera_id),\n","                'timestamp': timestamp,\n","                'image_url': str(f\"https://trafficcams.com/{camera_id}/{timestamp.strftime('%Y%m%d_%H%M%S')}.jpg\"),\n","                'vehicle_count': int(vehicles),\n","                'traffic_density': float(density),\n","                'average_speed': int(speed),\n","                'congestion_level': str(congestion),\n","                'processed': bool(True),\n","                'yolo_results': str(json.dumps(yolo_data))\n","            })\n","        \n","        print(f\"‚úÖ Generated {len(records)} traffic records\")\n","        return records\n","    \n","    def generate_historical_data(self, days=30):\n","        print(f\"üìÖ Generating {days} days of historical data...\")\n","        records = []\n","        start_date = datetime.now() - timedelta(days=days)\n","        zones = ['ZONE_A', 'ZONE_B', 'ZONE_C', 'ZONE_D', 'ZONE_E']\n","        weather_types = ['Sunny', 'Rainy', 'Cloudy']\n","        \n","        for zone in zones:\n","            for single_date in (start_date + timedelta(days=n) for n in range(days)):\n","                for hour in range(24):\n","                    is_weekend = single_date.weekday() >= 5\n","                    is_holiday = random.random() < 0.05  # 5% chance of holiday\n","                    \n","                    if is_holiday:\n","                        base_occ = random.uniform(0.7, 0.95)\n","                    elif is_weekend:\n","                        base_occ = random.uniform(0.4, 0.8)\n","                    else:\n","                        base_occ = random.uniform(0.3, 0.9)\n","                    \n","                    hour_mult = 1.0 if 8 <= hour <= 18 else 0.4\n","                    weather = random.choice(weather_types)\n","                    weather_impact = 0.8 if weather == 'Rainy' else 1.0\n","                    \n","                    occupancy = round(base_occ * hour_mult * weather_impact, 3)\n","                    volume = int(occupancy * random.randint(500, 2000))\n","                    \n","                    records.append({\n","                        'zone_id': str(zone),\n","                        'date': single_date.date(),\n","                        'hour': int(hour),\n","                        'average_occupancy': float(occupancy),\n","                        'traffic_volume': int(volume),\n","                        'weather_condition': str(weather),\n","                        'event_day': bool(is_holiday)\n","                    })\n","        \n","        print(f\"‚úÖ Generated {len(records)} historical records\")\n","        return records\n","\n","# Generate all datasets\n","generator = DataGenerator()\n","\n","print(\"üéØ GENERATING DATASETS...\")\n","parking_data = generator.generate_parking_data(100)  # Reduced for testing\n","traffic_data = generator.generate_traffic_data(50)   # Reduced for testing\n","historical_data = generator.generate_historical_data(7)  # Reduced for testing\n","\n","print(f\"\\nüìä GENERATION SUMMARY:\")\n","print(f\"   Parking Records: {len(parking_data)}\")\n","print(f\"   Traffic Records: {len(traffic_data)}\")\n","print(f\"   Historical Records: {len(historical_data)}\")\n","\n","# =============================================================================\n","# STEP 5: Load Data to Lakehouse (Fixed Version)\n","# =============================================================================\n","print(\"\\nüíæ STEP 5: LOADING DATA TO LAKEHOUSE...\")\n","\n","from pyspark.sql.types import (\n","    StructType, StructField, StringType, IntegerType, DoubleType, BooleanType, TimestampType, DateType\n",")\n","\n","# Define schemas explicitly to avoid Delta merge conflicts\n","parking_schema = StructType([\n","    StructField(\"sensor_id\", StringType(), True),\n","    StructField(\"timestamp\", TimestampType(), True),\n","    StructField(\"occupancy_status\", BooleanType(), True),\n","    StructField(\"vehicle_count\", IntegerType(), True),\n","    StructField(\"temperature\", DoubleType(), True),\n","    StructField(\"vibration_level\", DoubleType(), True),\n","    StructField(\"location\", StringType(), True),\n","    StructField(\"parking_zone\", StringType(), True),\n","    StructField(\"transaction_hash\", StringType(), True)\n","])\n","\n","traffic_schema = StructType([\n","    StructField(\"camera_id\", StringType(), True),\n","    StructField(\"timestamp\", TimestampType(), True),\n","    StructField(\"image_url\", StringType(), True),\n","    StructField(\"vehicle_count\", IntegerType(), True),\n","    StructField(\"traffic_density\", DoubleType(), True),\n","    StructField(\"average_speed\", IntegerType(), True),\n","    StructField(\"congestion_level\", StringType(), True),\n","    StructField(\"processed\", BooleanType(), True),\n","    StructField(\"yolo_results\", StringType(), True)\n","])\n","\n","historical_schema = StructType([\n","    StructField(\"zone_id\", StringType(), True),\n","    StructField(\"date\", DateType(), True),\n","    StructField(\"hour\", IntegerType(), True),\n","    StructField(\"average_occupancy\", DoubleType(), True),\n","    StructField(\"traffic_volume\", IntegerType(), True),\n","    StructField(\"weather_condition\", StringType(), True),\n","    StructField(\"event_day\", BooleanType(), True)\n","])\n","\n","print(\"üÖøÔ∏è Loading parking data...\")\n","try:\n","    parking_df = spark.createDataFrame(parking_data, schema=parking_schema)\n","    parking_df.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable(\"ParkingSensorData\")\n","    parking_count = spark.sql(\"SELECT COUNT(*) as cnt FROM ParkingSensorData\").collect()[0]['cnt']\n","    print(f\"   ‚úÖ ParkingSensorData: {parking_count} records\")\n","except Exception as e:\n","    print(f\"‚ùå Error loading parking data: {e}\")\n","\n","print(\"üö¶ Loading traffic data...\")\n","try:\n","    traffic_df = spark.createDataFrame(traffic_data, schema=traffic_schema)\n","    traffic_df.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable(\"TrafficCameraData\")\n","    traffic_count = spark.sql(\"SELECT COUNT(*) as cnt FROM TrafficCameraData\").collect()[0]['cnt']\n","    print(f\"   ‚úÖ TrafficCameraData: {traffic_count} records\")\n","except Exception as e:\n","    print(f\"‚ùå Error loading traffic data: {e}\")\n","\n","print(\"üìÖ Loading historical data...\")\n","try:\n","    historical_df = spark.createDataFrame(historical_data, schema=historical_schema)\n","    historical_df.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable(\"HistoricalTraffic\")\n","    historical_count = spark.sql(\"SELECT COUNT(*) as cnt FROM HistoricalTraffic\").collect()[0]['cnt']\n","    print(f\"   ‚úÖ HistoricalTraffic: {historical_count} records\")\n","except Exception as e:\n","    print(f\"‚ùå Error loading historical data: {e}\")\n","\n","print(\"üéâ Data loading completed successfully!\")\n","\n","\n","# =============================================================================\n","# STEP 6: Data Quality Validation\n","# =============================================================================\n","print(\"\\nüîç STEP 6: DATA QUALITY VALIDATION...\")\n","\n","# Check data statistics\n","validation_queries = {\n","    \"Parking Data Quality\": \"\"\"\n","        SELECT \n","            COUNT(*) as total_records,\n","            AVG(CASE WHEN occupancy_status = true THEN 1.0 ELSE 0.0 END) as occupancy_rate,\n","            COUNT(DISTINCT sensor_id) as unique_sensors,\n","            COUNT(DISTINCT parking_zone) as zones_covered,\n","            MIN(timestamp) as earliest_date,\n","            MAX(timestamp) as latest_date\n","        FROM ParkingSensorData\n","    \"\"\",\n","    \"Traffic Data Quality\": \"\"\"\n","        SELECT \n","            COUNT(*) as total_records,\n","            AVG(traffic_density) as avg_density,\n","            AVG(vehicle_count) as avg_vehicles,\n","            COUNT(DISTINCT camera_id) as unique_cameras,\n","            COUNT(DISTINCT congestion_level) as congestion_levels\n","        FROM TrafficCameraData\n","    \"\"\",\n","    \"Historical Data Quality\": \"\"\"\n","        SELECT \n","            COUNT(*) as total_records,\n","            AVG(average_occupancy) as avg_occupancy,\n","            AVG(traffic_volume) as avg_volume,\n","            COUNT(DISTINCT zone_id) as unique_zones,\n","            MIN(date) as start_date,\n","            MAX(date) as end_date\n","        FROM HistoricalTraffic\n","    \"\"\"\n","}\n","\n","for check_name, query in validation_queries.items():\n","    try:\n","        result = spark.sql(query).collect()[0]\n","        print(f\"\\n{check_name}:\")\n","        for key, value in result.asDict().items():\n","            if isinstance(value, float):\n","                print(f\"   {key}: {value:.3f}\")\n","            else:\n","                print(f\"   {key}: {value}\")\n","    except Exception as e:\n","        print(f\"‚ùå Error in {check_name}: {e}\")\n","\n","# =============================================================================\n","# FINAL SUMMARY\n","# =============================================================================\n","print(\"\\n\" + \"=\"*60)\n","print(\"üéâ REIMAGE-AI SMART PARKING - SETUP COMPLETED!\")\n","print(\"=\"*60)\n","\n","# Show final table status\n","print(\"\\nüìä FINAL TABLE STATUS:\")\n","try:\n","    tables = spark.sql(\"SHOW TABLES\").collect()\n","    for table in tables:\n","        try:\n","            count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {table['tableName']}\").collect()[0]['cnt']\n","            print(f\"   {table['tableName']}: {count} records\")\n","        except:\n","            print(f\"   {table['tableName']}: [Error counting]\")\n","except Exception as e:\n","    print(f\"‚ùå Error listing tables: {e}\")\n","\n","print(\"\\n‚úÖ NEXT STEPS:\")\n","print(\"   1. Run Notebook 2: AI Processing & Predictions\")\n","print(\"   2. Run Notebook 3: Monitoring & Dashboard\")\n","print(\"   3. Explore data in Lakehouse tables\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"f2d71779-6f3b-4a3f-b23b-b561d943aa3e","normalized_state":"finished","queued_time":"2025-10-10T17:02:32.2171598Z","session_start_time":null,"execution_start_time":"2025-10-10T17:02:32.2189564Z","execution_finish_time":"2025-10-10T17:03:29.504566Z","parent_msg_id":"10546c32-8759-447d-b795-328160d4ca49"},"text/plain":"StatementMeta(, f2d71779-6f3b-4a3f-b23b-b561d943aa3e, 6, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["üöÄ REIMAGE-AI SMART PARKING - COMPLETE SETUP\nüîß STEP 1: VERIFYING LAKEHOUSE CONNECTION...\n‚úÖ Lakehouse connected! Found 3 tables\n\nüßπ STEP 2: CLEANING UP EXISTING TABLES...\n‚úÖ Dropped table: ParkingSensorData\n‚úÖ Dropped table: TrafficCameraData\n‚úÖ Dropped table: HistoricalTraffic\n‚úÖ Dropped table: SensorMaintenanceData\n\nüìã STEP 3: CREATING FRESH TABLES...\n‚úÖ Created: ParkingSensorData\n‚úÖ Created: TrafficCameraData\n‚úÖ Created: HistoricalTraffic\nüéâ Tables created successfully!\n\nüöÄ STEP 4: GENERATING SYNTHETIC DATA...\nüéØ GENERATING DATASETS...\nüÖøÔ∏è Generating 100 parking records...\n‚úÖ Generated 100 parking records\nüö¶ Generating 50 traffic records...\n‚úÖ Generated 50 traffic records\nüìÖ Generating 7 days of historical data...\n‚úÖ Generated 840 historical records\n\nüìä GENERATION SUMMARY:\n   Parking Records: 100\n   Traffic Records: 50\n   Historical Records: 840\n\nüíæ STEP 5: LOADING DATA TO LAKEHOUSE...\nüÖøÔ∏è Loading parking data...\n   ‚úÖ ParkingSensorData: 100 records\nüö¶ Loading traffic data...\n   ‚úÖ TrafficCameraData: 50 records\nüìÖ Loading historical data...\n   ‚úÖ HistoricalTraffic: 840 records\nüéâ Data loading completed successfully!\n\nüîç STEP 6: DATA QUALITY VALIDATION...\n\nParking Data Quality:\n   total_records: 100\n   occupancy_rate: 0.35000\n   unique_sensors: 46\n   zones_covered: 5\n   earliest_date: 2025-09-10 22:03:51.181197\n   latest_date: 2025-10-10 11:34:04.253462\n\nTraffic Data Quality:\n   total_records: 50\n   avg_density: 0.502\n   avg_vehicles: 18.240\n   unique_cameras: 17\n   congestion_levels: 3\n\nHistorical Data Quality:\n   total_records: 840\n   avg_occupancy: 0.388\n   avg_volume: 480.318\n   unique_zones: 5\n   start_date: 2025-10-03\n   end_date: 2025-10-09\n\n============================================================\nüéâ REIMAGE-AI SMART PARKING - SETUP COMPLETED!\n============================================================\n\nüìä FINAL TABLE STATUS:\n   historicaltraffic: 840 records\n   parkingsensordata: 100 records\n   trafficcameradata: 50 records\n\n‚úÖ NEXT STEPS:\n   1. Run Notebook 2: AI Processing & Predictions\n   2. Run Notebook 3: Monitoring & Dashboard\n   3. Explore data in Lakehouse tables\n"]}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b4ee0930-071d-4f83-84ae-5222d4664ba0"},{"cell_type":"markdown","source":["#### ENHANCED IMPLEMENTATION WITH HEDERA, MCP & POWER BI"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"06371be1-cbe0-4d1d-8f53-3159269f00e3"},{"cell_type":"code","source":["# =============================================================\n","# üöÄ REIMAGE-AI SMART PARKING - WITH HEDERA BLOCKCHAIN\n","# =============================================================\n","\n","print(\"üöÄ REIMAGE-AI SMART PARKING - WITH HEDERA BLOCKCHAIN\")\n","\n","# =============================================================\n","# STEP 1: Verify Lakehouse Connection\n","# =============================================================\n","print(\"üîß STEP 1: VERIFYING LAKEHOUSE CONNECTION...\")\n","\n","try:\n","    tables = spark.sql(\"SHOW TABLES\").collect()\n","    print(f\"‚úÖ Lakehouse connected! Found {len(tables)} tables\")\n","except Exception as e:\n","    print(\"‚ùå No Lakehouse attached! Please attach a Lakehouse manually before running.\")\n","    raise e\n","\n","\n","# =============================================================\n","# STEP 2: Install Required Packages\n","# =============================================================\n","print(\"\\nüì¶ STEP 2: INSTALLING REQUIRED PACKAGES...\")\n","\n","try:\n","    %pip install faker hedera-sdk-py python-dotenv cryptography\n","    print(\"‚úÖ Packages installed successfully\")\n","except Exception as e:\n","    print(f\"‚ö†Ô∏è  Package installation note: {e}\")\n","\n","\n","# =============================================================\n","# STEP 3: Initialize Hedera Blockchain Manager\n","# =============================================================\n","print(\"\\n‚õìÔ∏è STEP 3: INITIALIZING HEDERA BLOCKCHAIN INTEGRATION...\")\n","\n","import hashlib\n","import json\n","from datetime import datetime, date, timedelta\n","import random\n","from faker import Faker\n","from pyspark.sql.types import *\n","\n","# ---- Custom JSON encoder for datetime ----\n","class EnhancedJSONEncoder(json.JSONEncoder):\n","    def default(self, obj):\n","        if isinstance(obj, (datetime, date)):\n","            return obj.isoformat()\n","        return super().default(obj)\n","\n","class HederaBlockchainManager:\n","    def __init__(self):\n","        self.hedera_config = {\n","            \"testnet_account_id\": \"0.0.12345\",\n","            \"testnet_private_key\": \"302e...\",\n","            \"topic_id\": None\n","        }\n","        print(\"üîê Hedera Manager Initialized (Test Mode)\")\n","\n","    def calculate_data_hash(self, data):\n","        data_str = json.dumps(data, cls=EnhancedJSONEncoder, sort_keys=True)\n","        return hashlib.sha256(data_str.encode()).hexdigest()\n","\n","    def simulate_hedera_transaction(self, data, transaction_type=\"DATA_STORAGE\"):\n","        data_hash = self.calculate_data_hash(data)\n","        timestamp = datetime.now().isoformat()\n","        receipt = {\n","            \"transaction_id\": f\"0.0.{int(datetime.now().timestamp())}\",\n","            \"data_hash\": data_hash,\n","            \"timestamp\": timestamp,\n","            \"transaction_type\": transaction_type,\n","            \"status\": \"SUCCESS\",\n","            \"consensus_timestamp\": timestamp,\n","            \"topic_id\": \"0.0.456789\",\n","            \"blockchain_verified\": True\n","        }\n","        print(f\"üîó Simulated Hedera Transaction {receipt['transaction_id']}\")\n","        return receipt\n","\n","    def store_on_blockchain(self, data, metadata=None):\n","        try:\n","            transaction_data = {\n","                \"data\": data,\n","                \"metadata\": metadata or {},\n","                \"storage_timestamp\": datetime.now().isoformat(),\n","                \"data_type\": \"parking_sensor\" if \"sensor_id\" in data else \"traffic_data\"\n","            }\n","            receipt = self.simulate_hedera_transaction(transaction_data)\n","            data_with_proof = data.copy()\n","            data_with_proof.update({\n","                \"blockchain_tx_id\": receipt[\"transaction_id\"],\n","                \"data_hash\": receipt[\"data_hash\"],\n","                \"blockchain_verified\": receipt[\"blockchain_verified\"]\n","            })\n","            return data_with_proof, receipt\n","        except Exception as e:\n","            print(f\"‚ùå Blockchain storage error: {e}\")\n","            return data, None\n","\n","hedera_manager = HederaBlockchainManager()\n","\n","\n","# =============================================================\n","# STEP 4: Create Enhanced Tables with Blockchain Fields\n","# =============================================================\n","print(\"\\nüìã STEP 4: CREATING ENHANCED TABLES WITH BLOCKCHAIN FIELDS...\")\n","\n","tables_to_clean = [\n","    \"ParkingSensorData\", \"TrafficCameraData\", \"HistoricalTraffic\",\n","    \"BlockchainTransactions\", \"ModelRegistry\", \"PowerBI_Metrics\"\n","]\n","for t in tables_to_clean:\n","    try:\n","        spark.sql(f\"DROP TABLE IF EXISTS {t}\")\n","        print(f\"‚úÖ Dropped table: {t}\")\n","    except:\n","        print(f\"‚ÑπÔ∏è  Could not drop {t}\")\n","\n","tables_sql = {\n","    \"ParkingSensorData\": \"\"\"\n","        CREATE TABLE ParkingSensorData (\n","            sensor_id STRING,\n","            timestamp TIMESTAMP,\n","            occupancy_status BOOLEAN,\n","            vehicle_count INT,\n","            temperature DOUBLE,\n","            vibration_level DOUBLE,\n","            location STRING,\n","            parking_zone STRING,\n","            transaction_hash STRING,\n","            blockchain_tx_id STRING,\n","            data_hash STRING,\n","            blockchain_verified BOOLEAN\n","        ) USING DELTA\n","    \"\"\",\n","    \"TrafficCameraData\": \"\"\"\n","        CREATE TABLE TrafficCameraData (\n","            camera_id STRING,\n","            timestamp TIMESTAMP,\n","            image_url STRING,\n","            vehicle_count INT,\n","            traffic_density DOUBLE,\n","            average_speed INT,\n","            congestion_level STRING,\n","            processed BOOLEAN,\n","            yolo_results STRING,\n","            blockchain_tx_id STRING,\n","            data_hash STRING,\n","            blockchain_verified BOOLEAN\n","        ) USING DELTA\n","    \"\"\",\n","    \"HistoricalTraffic\": \"\"\"\n","        CREATE TABLE HistoricalTraffic (\n","            zone_id STRING,\n","            date DATE,\n","            hour INT,\n","            average_occupancy DOUBLE,\n","            traffic_volume INT,\n","            weather_condition STRING,\n","            event_day BOOLEAN,\n","            data_hash STRING\n","        ) USING DELTA\n","    \"\"\",\n","    \"PowerBI_Metrics\": \"\"\"\n","        CREATE TABLE PowerBI_Metrics (\n","            metric_id STRING,\n","            metric_name STRING,\n","            metric_value DOUBLE,\n","            metric_timestamp TIMESTAMP,\n","            category STRING,\n","            zone_id STRING,\n","            data_source STRING\n","        ) USING DELTA\n","    \"\"\"\n","}\n","\n","for t, sql_stmt in tables_sql.items():\n","    spark.sql(sql_stmt)\n","    print(f\"‚úÖ Created: {t}\")\n","\n","print(\"üéâ Enhanced tables created successfully!\")\n","\n","\n","# =============================================================\n","# STEP 5: Generate Synthetic Data with Blockchain Integration\n","# =============================================================\n","print(\"\\nüöÄ STEP 5: GENERATING SYNTHETIC DATA WITH BLOCKCHAIN...\")\n","\n","fake = Faker()\n","\n","def generate_parking_data(num=100):\n","    records = []\n","    locations = [\"Downtown\", \"Airport\", \"Mall\", \"Hospital\"]\n","    zones = [\"ZONE_A\", \"ZONE_B\", \"ZONE_C\", \"ZONE_D\"]\n","    for _ in range(num):\n","        ts = fake.date_time_between(start_date=\"-30d\", end_date=\"now\")\n","        occupied = random.random() < (0.7 if 8 <= ts.hour <= 18 else 0.3)\n","        base = {\n","            \"sensor_id\": f\"SENSOR_{random.randint(1,50):03d}\",\n","            \"timestamp\": ts,\n","            \"occupancy_status\": occupied,\n","            \"vehicle_count\": 1 if occupied else 0,\n","            \"temperature\": round(random.uniform(15,35),2),\n","            \"vibration_level\": round(random.uniform(0.1,5.0),2),\n","            \"location\": random.choice(locations),\n","            \"parking_zone\": random.choice(zones),\n","            \"transaction_hash\": fake.sha256()\n","        }\n","        enhanced, receipt = hedera_manager.store_on_blockchain(base)\n","        records.append(enhanced)\n","    print(f\"‚úÖ Generated {len(records)} blockchain-backed parking records\")\n","    return records\n","\n","def generate_traffic_data(num=50):\n","    records = []\n","    for _ in range(num):\n","        ts = fake.date_time_between(start_date=\"-30d\", end_date=\"now\")\n","        rush = (7 <= ts.hour <= 9) or (16 <= ts.hour <= 18)\n","        vehicles = random.randint(20,50) if rush else random.randint(5,25)\n","        density = round(random.uniform(0.7,0.95),2) if rush else round(random.uniform(0.2,0.6),2)\n","        speed = random.randint(20,40) if rush else random.randint(40,60)\n","        congestion = \"HIGH\" if density>0.7 else \"MEDIUM\" if density>0.4 else \"LOW\"\n","        base = {\n","            \"camera_id\": f\"CAM_{random.randint(1,20):03d}\",\n","            \"timestamp\": ts,\n","            \"image_url\": f\"https://trafficcams.com/{ts.strftime('%Y%m%d_%H%M%S')}.jpg\",\n","            \"vehicle_count\": vehicles,\n","            \"traffic_density\": density,\n","            \"average_speed\": speed,\n","            \"congestion_level\": congestion,\n","            \"processed\": True,\n","            \"yolo_results\": json.dumps({\n","                \"vehicles_detected\": vehicles,\n","                \"confidence\": round(random.uniform(0.85,0.98),2)\n","            })\n","        }\n","        enhanced, receipt = hedera_manager.store_on_blockchain(base)\n","        records.append(enhanced)\n","    print(f\"‚úÖ Generated {len(records)} blockchain-backed traffic records\")\n","    return records\n","\n","def generate_historical_data(days=7):\n","    records=[]\n","    start=datetime.now()-timedelta(days=days)\n","    zones=[\"ZONE_A\",\"ZONE_B\",\"ZONE_C\",\"ZONE_D\"]\n","    weathers=[\"Sunny\",\"Rainy\",\"Cloudy\"]\n","    for zone in zones:\n","        for d in range(days):\n","            date_=start+timedelta(days=d)\n","            for hour in range(24):\n","                occ=round(random.uniform(0.3,0.95)*(1 if 8<=hour<=18 else 0.4),3)\n","                vol=int(occ*random.randint(500,2000))\n","                weather=random.choice(weathers)\n","                record={\n","                    \"zone_id\":zone,\n","                    \"date\":date_.date(),\n","                    \"hour\":hour,\n","                    \"average_occupancy\":occ,\n","                    \"traffic_volume\":vol,\n","                    \"weather_condition\":weather,\n","                    \"event_day\":random.random()<0.05,\n","                    \"data_hash\":hedera_manager.calculate_data_hash({\"z\":zone,\"d\":str(date_.date()),\"h\":hour})\n","                }\n","                records.append(record)\n","    print(f\"‚úÖ Generated {len(records)} historical records\")\n","    return records\n","\n","parking_data=generate_parking_data()\n","traffic_data=generate_traffic_data()\n","historical_data=generate_historical_data()\n","\n","\n","# =============================================================\n","# STEP 6: Load Data into Lakehouse\n","# =============================================================\n","print(\"\\nüíæ STEP 6: LOADING ENHANCED DATA TO LAKEHOUSE...\")\n","\n","parking_schema=StructType([\n","    StructField(\"sensor_id\",StringType()),StructField(\"timestamp\",TimestampType()),\n","    StructField(\"occupancy_status\",BooleanType()),StructField(\"vehicle_count\",IntegerType()),\n","    StructField(\"temperature\",DoubleType()),StructField(\"vibration_level\",DoubleType()),\n","    StructField(\"location\",StringType()),StructField(\"parking_zone\",StringType()),\n","    StructField(\"transaction_hash\",StringType()),StructField(\"blockchain_tx_id\",StringType()),\n","    StructField(\"data_hash\",StringType()),StructField(\"blockchain_verified\",BooleanType())\n","])\n","traffic_schema=StructType([\n","    StructField(\"camera_id\",StringType()),StructField(\"timestamp\",TimestampType()),\n","    StructField(\"image_url\",StringType()),StructField(\"vehicle_count\",IntegerType()),\n","    StructField(\"traffic_density\",DoubleType()),StructField(\"average_speed\",IntegerType()),\n","    StructField(\"congestion_level\",StringType()),StructField(\"processed\",BooleanType()),\n","    StructField(\"yolo_results\",StringType()),StructField(\"blockchain_tx_id\",StringType()),\n","    StructField(\"data_hash\",StringType()),StructField(\"blockchain_verified\",BooleanType())\n","])\n","hist_schema=StructType([\n","    StructField(\"zone_id\",StringType()),StructField(\"date\",DateType()),\n","    StructField(\"hour\",IntegerType()),StructField(\"average_occupancy\",DoubleType()),\n","    StructField(\"traffic_volume\",IntegerType()),StructField(\"weather_condition\",StringType()),\n","    StructField(\"event_day\",BooleanType()),StructField(\"data_hash\",StringType())\n","])\n","\n","spark.createDataFrame(parking_data,parking_schema)\\\n","     .write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\",\"true\").saveAsTable(\"ParkingSensorData\")\n","spark.createDataFrame(traffic_data,traffic_schema)\\\n","     .write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\",\"true\").saveAsTable(\"TrafficCameraData\")\n","spark.createDataFrame(historical_data,hist_schema)\\\n","     .write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\",\"true\").saveAsTable(\"HistoricalTraffic\")\n","\n","print(\"üéâ Enhanced data loading completed!\")\n","\n","\n","# =============================================================\n","# STEP 7: Generate Power BI Metrics\n","# =============================================================\n","print(\"\\nüìä STEP 7: GENERATING POWER BI METRICS...\")\n","\n","def safe_float(v): return float(v) if v else 0.0\n","\n","def generate_powerbi_metrics():\n","    metrics=[]\n","    now=datetime.now()\n","    try:\n","        blk=spark.sql(\"\"\"\n","            SELECT COUNT(*) as total,\n","                   SUM(CASE WHEN blockchain_verified THEN 1 ELSE 0 END) as verified,\n","                   AVG(CASE WHEN blockchain_verified THEN 1.0 ELSE 0.0 END) as rate\n","            FROM ParkingSensorData\n","        \"\"\").collect()[0]\n","        metrics.append({\"metric_id\":\"blk_total\",\"metric_name\":\"Total Blockchain Records\",\n","                        \"metric_value\":safe_float(blk[\"total\"]),\"metric_timestamp\":now,\n","                        \"category\":\"Blockchain\",\"zone_id\":\"ALL\",\"data_source\":\"Hedera\"})\n","        metrics.append({\"metric_id\":\"blk_rate\",\"metric_name\":\"Blockchain Verification Rate\",\n","                        \"metric_value\":safe_float(blk[\"rate\"]),\"metric_timestamp\":now,\n","                        \"category\":\"Blockchain\",\"zone_id\":\"ALL\",\"data_source\":\"Hedera\"})\n","    except Exception as e:\n","        print(f\"‚ùå Metric generation error: {e}\")\n","    return metrics\n","\n","metrics=generate_powerbi_metrics()\n","if metrics:\n","    spark.createDataFrame(metrics).write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"PowerBI_Metrics\")\n","    print(f\"‚úÖ Generated {len(metrics)} Power BI metrics\")\n","\n","# =============================================================\n","# FINAL SUMMARY\n","# =============================================================\n","print(\"\\n\" + \"=\"*60)\n","print(\"üéâ REIMAGE-AI SMART PARKING WITH HEDERA - SETUP COMPLETED!\")\n","print(\"=\"*60)\n","\n","for t in [\"ParkingSensorData\",\"TrafficCameraData\",\"HistoricalTraffic\",\"PowerBI_Metrics\"]:\n","    c=spark.sql(f\"SELECT COUNT(*) as cnt FROM {t}\").collect()[0]['cnt']\n","    print(f\"   {t}: {c} records\")\n","\n","print(\"\\n‚úÖ NEXT STEPS:\")\n","print(\"   1. Run Notebook 2 - AI Processing & MCP Integration\")\n","print(\"   2. Run Notebook 3 - Power BI Dashboard & Monitoring\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":23,"statement_ids":[18,19,20,21,22,23],"state":"finished","livy_statement_state":"available","session_id":"d0b4adbe-9011-4d4d-acd3-20483e4fe494","normalized_state":"finished","queued_time":"2025-10-10T17:51:46.5732529Z","session_start_time":null,"execution_start_time":"2025-10-10T17:51:47.8640481Z","execution_finish_time":"2025-10-10T17:52:46.6809535Z","parent_msg_id":"6a9750fd-259d-4b29-a5ba-a933b92c143a"},"text/plain":"StatementMeta(, d0b4adbe-9011-4d4d-acd3-20483e4fe494, 23, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: faker in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (37.11.0)\nRequirement already satisfied: hedera-sdk-py in /nfs4/pyenv-0ae02918-bb11-432b-86b5-dbae1e3dcb87/lib/python3.11/site-packages (2.50.0)\nRequirement already satisfied: python-dotenv in /nfs4/pyenv-0ae02918-bb11-432b-86b5-dbae1e3dcb87/lib/python3.11/site-packages (1.1.1)\nRequirement already satisfied: cryptography in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (42.0.2)\nRequirement already satisfied: tzdata in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from faker) (2023.3)\nRequirement already satisfied: pyjnius>=1.6.1 in /nfs4/pyenv-0ae02918-bb11-432b-86b5-dbae1e3dcb87/lib/python3.11/site-packages (from hedera-sdk-py) (1.7.0)\nRequirement already satisfied: cffi>=1.12 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from cryptography) (1.16.0)\nRequirement already satisfied: pycparser in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from cffi>=1.12->cryptography) (2.21)\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\nüöÄ REIMAGE-AI SMART PARKING - WITH HEDERA BLOCKCHAIN\nüîß STEP 1: VERIFYING LAKEHOUSE CONNECTION...\n‚úÖ Lakehouse connected! Found 11 tables\n\nüì¶ STEP 2: INSTALLING REQUIRED PACKAGES...\n‚úÖ Packages installed successfully\n\n‚õìÔ∏è STEP 3: INITIALIZING HEDERA BLOCKCHAIN INTEGRATION...\nüîê Hedera Manager Initialized (Test Mode)\n\nüìã STEP 4: CREATING ENHANCED TABLES WITH BLOCKCHAIN FIELDS...\n‚úÖ Dropped table: ParkingSensorData\n‚úÖ Dropped table: TrafficCameraData\n‚úÖ Dropped table: HistoricalTraffic\n‚úÖ Dropped table: BlockchainTransactions\n‚úÖ Dropped table: ModelRegistry\n‚úÖ Dropped table: PowerBI_Metrics\n‚úÖ Created: ParkingSensorData\n‚úÖ Created: TrafficCameraData\n‚úÖ Created: HistoricalTraffic\n‚úÖ Created: PowerBI_Metrics\nüéâ Enhanced tables created successfully!\n\nüöÄ STEP 5: GENERATING SYNTHETIC DATA WITH BLOCKCHAIN...\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\n‚úÖ Generated 100 blockchain-backed parking records\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\nüîó Simulated Hedera Transaction 0.0.1760118728\n‚úÖ Generated 50 blockchain-backed traffic records\n‚úÖ Generated 672 historical records\n\nüíæ STEP 6: LOADING ENHANCED DATA TO LAKEHOUSE...\nüéâ Enhanced data loading completed!\n\nüìä STEP 7: GENERATING POWER BI METRICS...\n‚úÖ Generated 2 Power BI metrics\n\n============================================================\nüéâ REIMAGE-AI SMART PARKING WITH HEDERA - SETUP COMPLETED!\n============================================================\n   ParkingSensorData: 100 records\n   TrafficCameraData: 50 records\n   HistoricalTraffic: 672 records\n   PowerBI_Metrics: 2 records\n\n‚úÖ NEXT STEPS:\n   1. Run Notebook 2 - AI Processing & MCP Integration\n   2. Run Notebook 3 - Power BI Dashboard & Monitoring\nWarning: PySpark kernel has been restarted to use updated packages.\n\n"]}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5d1bed84-80f5-48af-9344-0fe3161ab9e6"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"06b86e1a-668d-4d65-a5f0-7ba8a61fa57d"}],"default_lakehouse":"06b86e1a-668d-4d65-a5f0-7ba8a61fa57d","default_lakehouse_name":"ParkingDataLakehouse","default_lakehouse_workspace_id":"d3afc09a-dc29-418e-be57-836e9d2cc5f1"}}},"nbformat":4,"nbformat_minor":5}
{"cells":[{"cell_type":"code","source":["# AI PROCESSING & PREDICTIONS - RUN AFTER NOTEBOOK 1\n","# Make sure same Lakehouse is attached!\n","\n","print(\"ü§ñ REIMAGE-AI SMART PARKING - AI PROCESSING\")\n","\n","# =============================================================================\n","# STEP 1: Verify Data Exists\n","# =============================================================================\n","print(\"üîç STEP 1: VERIFYING DATA AVAILABILITY...\")\n","\n","required_tables = [\"ParkingSensorData\", \"TrafficCameraData\", \"HistoricalTraffic\"]\n","available_tables = []\n","\n","for table in required_tables:\n","    try:\n","        count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {table}\").collect()[0]['cnt']\n","        available_tables.append((table, count))\n","        print(f\"‚úÖ {table}: {count} records\")\n","    except:\n","        print(f\"‚ùå {table}: Not available\")\n","\n","if len(available_tables) < len(required_tables):\n","    print(\"‚ö†Ô∏è Some required tables are missing. Please run Notebook 1 first.\")\n","\n","# =============================================================================\n","# STEP 2: YOLO Image Processing\n","# =============================================================================\n","print(\"\\nüñºÔ∏è STEP 2: YOLO IMAGE PROCESSING...\")\n","\n","from datetime import datetime\n","import json, random\n","from pyspark.sql.types import (\n","    StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",")\n","\n","try:\n","    traffic_df = spark.sql(\"\"\"\n","        SELECT * FROM TrafficCameraData \n","        ORDER BY timestamp DESC\n","        LIMIT 20\n","    \"\"\")\n","\n","    print(f\"üîç Processing {traffic_df.count()} traffic images...\")\n","\n","    yolo_results = []\n","    for row in traffic_df.collect():\n","        original_count = int(row['vehicle_count'])\n","        yolo_count = max(0, original_count + random.randint(-2, 2))\n","        confidence = round(random.uniform(0.85, 0.97), 2)\n","\n","        vehicle_breakdown = {\n","            'cars': max(0, yolo_count - random.randint(0, 3)),\n","            'trucks': random.randint(0, 2),\n","            'motorcycles': random.randint(0, 2),\n","            'buses': random.randint(0, 1)\n","        }\n","\n","        yolo_analysis = {\n","            'original_detection': original_count,\n","            'yolo_detection': yolo_count,\n","            'confidence_score': confidence,\n","            'processing_time_ms': random.randint(80, 200),\n","            'model_version': 'yolov8n-parking',\n","            'vehicle_breakdown': vehicle_breakdown,\n","            'image_quality': random.choice(['HIGH', 'MEDIUM', 'LOW']),\n","            'detection_quality': 'EXCELLENT' if confidence > 0.9 else 'GOOD'\n","        }\n","\n","        yolo_results.append({\n","            'camera_id': str(row['camera_id']),\n","            'timestamp': row['timestamp'],\n","            'original_vehicle_count': int(original_count),\n","            'yolo_vehicle_count': int(yolo_count),\n","            'processing_confidence': float(confidence),\n","            'vehicle_breakdown': json.dumps(vehicle_breakdown),\n","            'yolo_analysis': json.dumps(yolo_analysis),\n","            'processed_at': datetime.now(),\n","            'processing_status': 'COMPLETED'\n","        })\n","\n","    if yolo_results:\n","        print(\"üíæ Saving YOLO processing results...\")\n","\n","        # Force consistent schema ‚Äî drop and recreate each run\n","        spark.sql(\"DROP TABLE IF EXISTS YOLOProcessedData\")\n","\n","        yolo_schema = StructType([\n","            StructField(\"camera_id\", StringType()),\n","            StructField(\"timestamp\", TimestampType()),\n","            StructField(\"original_vehicle_count\", IntegerType()),\n","            StructField(\"yolo_vehicle_count\", IntegerType()),\n","            StructField(\"processing_confidence\", DoubleType()),\n","            StructField(\"vehicle_breakdown\", StringType()),\n","            StructField(\"yolo_analysis\", StringType()),\n","            StructField(\"processed_at\", TimestampType()),\n","            StructField(\"processing_status\", StringType())\n","        ])\n","\n","        yolo_df = spark.createDataFrame(yolo_results, schema=yolo_schema)\n","        yolo_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"YOLOProcessedData\")\n","\n","        saved_count = spark.sql(\"SELECT COUNT(*) as cnt FROM YOLOProcessedData\").collect()[0]['cnt']\n","        print(f\"‚úÖ Saved {saved_count} YOLO processing records\")\n","\n","        print(\"\\nüìä YOLO PROCESSING RESULTS SAMPLE:\")\n","        yolo_df.select(\"camera_id\", \"original_vehicle_count\", \"yolo_vehicle_count\", \"processing_confidence\").show(10)\n","    else:\n","        print(\"‚ö†Ô∏è No YOLO results to save.\")\n","\n","except Exception as e:\n","    print(f\"‚ùå Error in YOLO processing: {e}\")\n","\n","# =============================================================================\n","# STEP 3: RAG Traffic Predictions\n","# =============================================================================\n","print(\"\\nüîÆ STEP 3: RAG TRAFFIC PREDICTIONS...\")\n","\n","try:\n","    print(\"üìö Building knowledge base from historical patterns...\")\n","    historical_data = spark.sql(\"SELECT * FROM HistoricalTraffic\").toPandas()\n","\n","    knowledge_base = {}\n","    for _, row in historical_data.iterrows():\n","        key = f\"{row['zone_id']}_{row['hour']:02d}_{row['weather_condition']}\"\n","        knowledge_base.setdefault(key, []).append({\n","            'occupancy': row['average_occupancy'],\n","            'volume': row['traffic_volume'],\n","            'is_event': row['event_day']\n","        })\n","\n","    print(f\"‚úÖ Knowledge base built with {len(knowledge_base)} patterns\")\n","\n","    print(\"üéØ Generating predictions for next 24 hours...\")\n","    predictions = []\n","    zones = ['ZONE_A', 'ZONE_B', 'ZONE_C', 'ZONE_D', 'ZONE_E']\n","    weather_conditions = ['Sunny', 'Rainy', 'Cloudy']\n","\n","    for zone in zones:\n","        for hour in range(24):\n","            weather = random.choice(weather_conditions)\n","            is_event = random.random() < 0.1\n","            key = f\"{zone}_{hour:02d}_{weather}\"\n","            patterns = knowledge_base.get(key, [])\n","\n","            if patterns:\n","                occs = [p['occupancy'] for p in patterns if p['is_event'] == is_event]\n","                predicted = sum(occs)/len(occs) if occs else 0.5\n","                conf = min(0.95, len(occs)*0.1)\n","            else:\n","                predicted, conf = 0.5, 0.3\n","\n","            predicted += random.uniform(-0.05, 0.05)\n","            predicted = max(0, min(1, predicted))\n","\n","            predictions.append({\n","                'zone_id': zone,\n","                'target_hour': hour,\n","                'predicted_occupancy': round(predicted, 3),\n","                'confidence': round(conf, 3),\n","                'weather_condition': weather,\n","                'is_event_day': is_event,\n","                'similar_patterns_used': len(patterns),\n","                'prediction_time': datetime.now()\n","            })\n","\n","    if predictions:\n","        print(\"üíæ Saving traffic predictions...\")\n","\n","        spark.sql(\"DROP TABLE IF EXISTS TrafficPredictions\")\n","\n","        from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, BooleanType, TimestampType\n","        pred_schema = StructType([\n","            StructField(\"zone_id\", StringType()),\n","            StructField(\"target_hour\", IntegerType()),\n","            StructField(\"predicted_occupancy\", DoubleType()),\n","            StructField(\"confidence\", DoubleType()),\n","            StructField(\"weather_condition\", StringType()),\n","            StructField(\"is_event_day\", BooleanType()),\n","            StructField(\"similar_patterns_used\", IntegerType()),\n","            StructField(\"prediction_time\", TimestampType())\n","        ])\n","\n","        predictions_df = spark.createDataFrame(predictions, schema=pred_schema)\n","        predictions_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"TrafficPredictions\")\n","\n","        saved_count = spark.sql(\"SELECT COUNT(*) as cnt FROM TrafficPredictions\").collect()[0]['cnt']\n","        print(f\"‚úÖ Saved {saved_count} predictions\")\n","\n","        print(\"\\nüìä PREDICTION RESULTS SAMPLE:\")\n","        predictions_df.select(\"zone_id\", \"target_hour\", \"predicted_occupancy\", \"confidence\").show(10)\n","    else:\n","        print(\"‚ö†Ô∏è No predictions to save.\")\n","\n","except Exception as e:\n","    print(f\"‚ùå Error in prediction system: {e}\")\n","\n","# =============================================================================\n","# STEP 4: AI PERFORMANCE ANALYSIS\n","# =============================================================================\n","print(\"\\nüìà STEP 4: AI PERFORMANCE ANALYSIS...\")\n","\n","try:\n","    yolo_metrics = spark.sql(\"\"\"\n","        SELECT \n","            AVG(processing_confidence) as avg_confidence,\n","            AVG(ABS(original_vehicle_count - yolo_vehicle_count)) as avg_diff,\n","            COUNT(*) as total_processed\n","        FROM YOLOProcessedData\n","    \"\"\").collect()[0]\n","    print(\"üñºÔ∏è YOLO PERFORMANCE:\")\n","    print(f\"   Avg Confidence: {yolo_metrics['avg_confidence']:.3f}\")\n","    print(f\"   Avg Count Diff: {yolo_metrics['avg_diff']:.2f}\")\n","    print(f\"   Total Processed: {yolo_metrics['total_processed']}\")\n","except:\n","    print(\"üñºÔ∏è YOLO PERFORMANCE: No data available\")\n","\n","try:\n","    pred_metrics = spark.sql(\"\"\"\n","        SELECT \n","            AVG(confidence) as avg_confidence,\n","            COUNT(*) as total_predictions\n","        FROM TrafficPredictions\n","    \"\"\").collect()[0]\n","    print(\"\\nüîÆ PREDICTION PERFORMANCE:\")\n","    print(f\"   Avg Confidence: {pred_metrics['avg_confidence']:.3f}\")\n","    print(f\"   Total Predictions: {pred_metrics['total_predictions']}\")\n","except:\n","    print(\"üîÆ PREDICTION PERFORMANCE: No data available\")\n","\n","# =============================================================================\n","# FINAL SUMMARY\n","# =============================================================================\n","print(\"\\n\" + \"=\"*60)\n","print(\"üéâ AI PROCESSING COMPLETED!\")\n","print(\"=\"*60)\n","\n","for table in [\"YOLOProcessedData\", \"TrafficPredictions\"]:\n","    try:\n","        cnt = spark.sql(f\"SELECT COUNT(*) as cnt FROM {table}\").collect()[0]['cnt']\n","        print(f\"   {table}: {cnt} records\")\n","    except:\n","        print(f\"   {table}: Not available\")\n","\n","print(\"\\n‚úÖ NEXT STEP: Run Notebook 3 for Monitoring & Dashboard\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"332fd53d-b5dd-4a9a-9d98-f62d0c41c5e8","normalized_state":"finished","queued_time":"2025-10-10T17:11:22.6322304Z","session_start_time":null,"execution_start_time":"2025-10-10T17:11:22.6337095Z","execution_finish_time":"2025-10-10T17:12:14.5174364Z","parent_msg_id":"60c8173d-24c1-4019-a723-bb518e650323"},"text/plain":"StatementMeta(, 332fd53d-b5dd-4a9a-9d98-f62d0c41c5e8, 4, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["ü§ñ REIMAGE-AI SMART PARKING - AI PROCESSING\nüîç STEP 1: VERIFYING DATA AVAILABILITY...\n‚úÖ ParkingSensorData: 100 records\n‚úÖ TrafficCameraData: 50 records\n‚úÖ HistoricalTraffic: 840 records\n\nüñºÔ∏è STEP 2: YOLO IMAGE PROCESSING...\nüîç Processing 20 traffic images...\nüíæ Saving YOLO processing results...\n‚úÖ Saved 20 YOLO processing records\n\nüìä YOLO PROCESSING RESULTS SAMPLE:\n+---------+----------------------+------------------+---------------------+\n|camera_id|original_vehicle_count|yolo_vehicle_count|processing_confidence|\n+---------+----------------------+------------------+---------------------+\n|  CAM_004|                     9|                11|                 0.95|\n|  CAM_005|                    48|                50|                 0.89|\n|  CAM_002|                    17|                16|                 0.92|\n|  CAM_012|                    18|                17|                 0.96|\n|  CAM_004|                    10|                10|                  0.9|\n|  CAM_005|                     7|                 8|                 0.92|\n|  CAM_014|                    16|                17|                 0.85|\n|  CAM_015|                     9|                 7|                  0.9|\n|  CAM_011|                    44|                42|                 0.87|\n|  CAM_004|                     5|                 7|                 0.93|\n+---------+----------------------+------------------+---------------------+\nonly showing top 10 rows\n\n\nüîÆ STEP 3: RAG TRAFFIC PREDICTIONS...\nüìö Building knowledge base from historical patterns...\n‚úÖ Knowledge base built with 339 patterns\nüéØ Generating predictions for next 24 hours...\nüíæ Saving traffic predictions...\n‚úÖ Saved 120 predictions\n\nüìä PREDICTION RESULTS SAMPLE:\n+-------+-----------+-------------------+----------+\n|zone_id|target_hour|predicted_occupancy|confidence|\n+-------+-----------+-------------------+----------+\n| ZONE_A|          0|              0.299|       0.3|\n| ZONE_A|          1|              0.364|       0.1|\n| ZONE_A|          2|              0.306|       0.2|\n| ZONE_A|          3|              0.305|       0.3|\n| ZONE_A|          4|              0.191|       0.2|\n| ZONE_A|          5|              0.244|       0.4|\n| ZONE_A|          6|              0.181|       0.3|\n| ZONE_A|          7|              0.284|       0.2|\n| ZONE_A|          8|              0.443|       0.2|\n| ZONE_A|          9|              0.336|       0.1|\n+-------+-----------+-------------------+----------+\nonly showing top 10 rows\n\n\nüìà STEP 4: AI PERFORMANCE ANALYSIS...\nüñºÔ∏è YOLO PERFORMANCE:\n   Avg Confidence: 0.911\n   Avg Count Diff: 1.20\n   Total Processed: 20\n\nüîÆ PREDICTION PERFORMANCE:\n   Avg Confidence: 0.214\n   Total Predictions: 120\n\n============================================================\nüéâ AI PROCESSING COMPLETED!\n============================================================\n   YOLOProcessedData: 20 records\n   TrafficPredictions: 120 records\n\n‚úÖ NEXT STEP: Run Notebook 3 for Monitoring & Dashboard\n"]}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cfeacdd1-94c5-416c-8774-c51ed8cb3cec"},{"cell_type":"markdown","source":["#### ENHANCED IMPLEMENTATION WITH HEDERA, MCP & POWER BI"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"563f252a-0880-4f46-8779-04718f732359"},{"cell_type":"code","source":["# =============================================================\n","# ü§ñ REIMAGE-AI SMART PARKING - AI PROCESSING WITH MCP\n","# =============================================================\n","print(\"ü§ñ REIMAGE-AI SMART PARKING - AI PROCESSING WITH MCP\")\n","\n","# =============================================================\n","# STEP 0: RE-INITIALIZE HEDERA MANAGER (needed if run standalone)\n","# =============================================================\n","print(\"‚öôÔ∏è Re-initializing Hedera Blockchain Manager...\")\n","\n","import hashlib, json, os, random\n","from datetime import datetime, date\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","class EnhancedJSONEncoder(json.JSONEncoder):\n","    def default(self, obj):\n","        if isinstance(obj, (datetime, date)):\n","            return obj.isoformat()\n","        return super().default(obj)\n","\n","class HederaBlockchainManager:\n","    def __init__(self):\n","        self.hedera_config = {\n","            \"testnet_account_id\": \"0.0.12345\",\n","            \"testnet_private_key\": \"302e...\",\n","            \"topic_id\": \"0.0.456789\"\n","        }\n","        print(\"üîê Hedera Manager initialized (Test Mode)\")\n","\n","    def calculate_data_hash(self, data):\n","        data_str = json.dumps(data, cls=EnhancedJSONEncoder, sort_keys=True)\n","        return hashlib.sha256(data_str.encode()).hexdigest()\n","\n","    def simulate_hedera_transaction(self, data, tx_type=\"DATA_STORAGE\"):\n","        data_hash = self.calculate_data_hash(data)\n","        ts = datetime.now().isoformat()\n","        receipt = {\n","            \"transaction_id\": f\"0.0.{int(datetime.now().timestamp())}\",\n","            \"data_hash\": data_hash,\n","            \"timestamp\": ts,\n","            \"transaction_type\": tx_type,\n","            \"status\": \"SUCCESS\",\n","            \"topic_id\": self.hedera_config[\"topic_id\"],\n","            \"blockchain_verified\": True\n","        }\n","        return receipt\n","\n","    def store_on_blockchain(self, data, metadata=None):\n","        tx_data = {\n","            \"data\": data,\n","            \"metadata\": metadata or {},\n","            \"storage_timestamp\": datetime.now().isoformat()\n","        }\n","        receipt = self.simulate_hedera_transaction(tx_data)\n","        stored = data.copy()\n","        stored.update({\n","            \"blockchain_tx_id\": receipt[\"transaction_id\"],\n","            \"data_hash\": receipt[\"data_hash\"],\n","            \"blockchain_verified\": True\n","        })\n","        return stored, receipt\n","\n","# ‚úÖ instantiate\n","hedera_manager = HederaBlockchainManager()\n","\n","\n","# =============================================================\n","# STEP 1: MODEL CONTEXT PROTOCOL (MCP) IMPLEMENTATION\n","# =============================================================\n","print(\"‚öñÔ∏è STEP 1: MODEL CONTEXT PROTOCOL (MCP) IMPLEMENTATION...\")\n","\n","from datetime import datetime\n","import hashlib, json\n","\n","class ModelContextProtocol:\n","    def __init__(self, hedera_manager):\n","        self.hedera = hedera_manager\n","        self.model_registry = {}\n","        \n","    def register_model(self, model_name, model_type, version, performance_metrics, description=\"\"):\n","        model_id = f\"{model_name}_v{version}\"\n","        info = {\n","            \"model_id\": model_id,\n","            \"model_name\": model_name,\n","            \"model_type\": model_type,\n","            \"version\": version,\n","            \"performance_metrics\": performance_metrics,\n","            \"description\": description,\n","            \"registered_date\": datetime.now(),\n","            \"status\": \"STAGING\",\n","            \"last_updated\": datetime.now(),\n","            \"model_hash\": self._hash(performance_metrics)\n","        }\n","        stored, receipt = self.hedera.store_on_blockchain(\n","            info, metadata={\"model_id\": model_id, \"registration_type\": \"MODEL_DEPLOYMENT\"}\n","        )\n","        if receipt:\n","            info[\"blockchain_tx_id\"] = receipt[\"transaction_id\"]\n","        self.model_registry[model_id] = info\n","        self._save(info)\n","        print(f\"‚úÖ Registered model {model_id}\")\n","        return model_id\n","\n","    def deploy_model(self, model_id, env=\"PRODUCTION\"):\n","        if model_id not in self.model_registry:\n","            print(f\"‚ùå Model {model_id} not found\"); return False\n","        m = self.model_registry[model_id]\n","        if env == \"PRODUCTION\":\n","            for k in (\"accuracy\",\"precision\",\"recall\"):\n","                if m[\"performance_metrics\"].get(k,0) < 0.8:\n","                    print(f\"‚ùå {model_id} below threshold {k}\"); return False\n","        m[\"status\"]=env; m[\"deployed_date\"]=datetime.now()\n","        self.hedera.store_on_blockchain(\n","            {\"model_id\":model_id,\"env\":env,\"deployment_time\":datetime.now().isoformat()},\n","            metadata={\"deployment_type\":\"MODEL_DEPLOYMENT\"}\n","        )\n","        self._save(m)\n","        print(f\"üöÄ Deployed {model_id} ‚Üí {env}\")\n","        return True\n","\n","    def log_inference(self, model_id, input_data, output_data, conf):\n","        data={\"model_id\":model_id,\"input\":input_data,\"output\":output_data,\n","              \"confidence\":conf,\"timestamp\":datetime.now().isoformat()}\n","        stored, r=self.hedera.store_on_blockchain(\n","            data,metadata={\"inference_type\":\"MODEL_PREDICTION\"}\n","        )\n","        return r\n","\n","    def _hash(self, data): return hashlib.sha256(json.dumps(data,sort_keys=True).encode()).hexdigest()\n","    def _save(self, info):\n","        try:\n","            spark.createDataFrame([info]).write.mode(\"append\").format(\"delta\").saveAsTable(\"ModelRegistry\")\n","        except Exception as e:\n","            print(f\"‚ö†Ô∏è Registry save error: {e}\")\n","\n","# ‚úÖ Initialize MCP\n","mcp = ModelContextProtocol(hedera_manager)\n","print(\"‚úÖ MCP initialized with Hedera integration\\n\")\n","\n","# =============================================================================\n","# STEP 2: Register AI Models with MCP\n","# =============================================================================\n","print(\"\\nüìù STEP 2: REGISTERING AI MODELS WITH MCP GOVERNANCE...\")\n","\n","# Define models to register\n","models_to_register = [\n","    {\n","        'name': 'YOLO_Parking_Detection',\n","        'type': 'Computer_Vision',\n","        'version': '2.1',\n","        'metrics': {\n","            'accuracy': 0.94,\n","            'precision': 0.92,\n","            'recall': 0.93,\n","            'f1_score': 0.925,\n","            'inference_time_ms': 150,\n","            'training_samples': 50000,\n","            'model_size_mb': 45.2\n","        },\n","        'description': 'YOLO-based vehicle detection for parking spaces with blockchain audit'\n","    },\n","    {\n","        'name': 'Traffic_Prediction_RAG',\n","        'type': 'ML_Prediction', \n","        'version': '1.2',\n","        'metrics': {\n","            'accuracy': 0.87,\n","            'precision': 0.85,\n","            'recall': 0.86,\n","            'mse': 0.023,\n","            'mae': 0.045,\n","            'r_squared': 0.82,\n","            'training_hours': 24\n","        },\n","        'description': 'RAG-based traffic pattern prediction with historical context'\n","    },\n","    {\n","        'name': 'Parking_Occupancy_Predictor',\n","        'type': 'Time_Series_Forecasting',\n","        'version': '1.1',\n","        'metrics': {\n","            'accuracy': 0.91,\n","            'precision': 0.89,\n","            'recall': 0.90,\n","            'mae': 0.045,\n","            'rmse': 0.067,\n","            'training_epochs': 100\n","        },\n","        'description': 'LSTM-based parking occupancy prediction with real-time adaptation'\n","    }\n","]\n","\n","print(\"üîß Registering models with full governance...\")\n","registered_models = []\n","for model_config in models_to_register:\n","    model_id = mcp.register_model(\n","        model_config['name'],\n","        model_config['type'],\n","        model_config['version'],\n","        model_config['metrics'],\n","        model_config['description']\n","    )\n","    registered_models.append(model_id)\n","\n","# Deploy models to appropriate environments\n","print(\"\\nüöÄ DEPLOYING MODELS...\")\n","if registered_models:\n","    # Deploy first model to production\n","    mcp.deploy_model(registered_models[0], 'PRODUCTION')\n","    # Deploy second model to staging\n","    mcp.deploy_model(registered_models[1], 'STAGING')\n","    # Keep third in development\n","    mcp.deploy_model(registered_models[2], 'DEVELOPMENT')\n","\n","# =============================================================================\n","# STEP 3: Enhanced YOLO Processing with MCP\n","# =============================================================================\n","print(\"\\nüñºÔ∏è STEP 3: ENHANCED YOLO PROCESSING WITH MCP...\")\n","\n","import random\n","\n","class MCPEnhancedYOLOProcessor:\n","    def __init__(self, mcp_system):\n","        self.mcp = mcp_system\n","        self.yolo_model_id = \"YOLO_Parking_Detection_v2.1\"\n","    \n","    def process_with_governance(self):\n","        \"\"\"Process images with full MCP governance\"\"\"\n","        print(\"üéØ Processing traffic images with MCP governance...\")\n","        \n","        try:\n","            # Get traffic data for processing\n","            traffic_df = spark.sql(\"\"\"\n","                SELECT * FROM TrafficCameraData \n","                ORDER BY timestamp DESC\n","                LIMIT 15\n","            \"\"\")\n","\n","            processed_results = []\n","            total_confidence = 0\n","            processed_count = 0\n","\n","            for row in traffic_df.collect():\n","                # Enhanced YOLO processing simulation\n","                original_count = row['vehicle_count']\n","                yolo_count = max(0, original_count + random.randint(-2, 2))\n","                confidence = round(random.uniform(0.85, 0.97), 2)\n","                \n","                # Log inference with MCP\n","                inference_data = {\n","                    'camera_id': row['camera_id'],\n","                    'original_count': original_count,\n","                    'processed_count': yolo_count\n","                }\n","                \n","                output_data = {\n","                    'yolo_count': yolo_count,\n","                    'confidence': confidence,\n","                    'processing_time_ms': random.randint(80, 200)\n","                }\n","                \n","                # Log to MCP with blockchain\n","                receipt = self.mcp.log_inference(\n","                    self.yolo_model_id,\n","                    inference_data,\n","                    output_data,\n","                    confidence\n","                )\n","                \n","                # Enhanced results with MCP metadata\n","                yolo_record = {\n","                    'camera_id': str(row['camera_id']),\n","                    'timestamp': row['timestamp'],\n","                    'original_vehicle_count': int(original_count),\n","                    'yolo_vehicle_count': int(yolo_count),\n","                    'processing_confidence': float(confidence),\n","                    'model_id': self.yolo_model_id,\n","                    'mcp_inference_id': receipt['transaction_id'] if receipt else None,\n","                    'processed_at': datetime.now(),\n","                    'processing_status': 'COMPLETED_WITH_MCP'\n","                }\n","                processed_results.append(yolo_record)\n","                \n","                total_confidence += confidence\n","                processed_count += 1\n","\n","            # Save results\n","            if processed_results:\n","                spark.sql(\"DROP TABLE IF EXISTS YOLOProcessedData\")\n","                spark.sql(\"\"\"\n","                    CREATE TABLE YOLOProcessedData (\n","                        camera_id STRING,\n","                        timestamp TIMESTAMP,\n","                        original_vehicle_count INT,\n","                        yolo_vehicle_count INT,\n","                        processing_confidence DOUBLE,\n","                        model_id STRING,\n","                        mcp_inference_id STRING,\n","                        processed_at TIMESTAMP,\n","                        processing_status STRING\n","                    ) USING DELTA\n","                \"\"\")\n","                \n","                yolo_df = spark.createDataFrame(processed_results)\n","                yolo_df.write.mode(\"append\").format(\"delta\").saveAsTable(\"YOLOProcessedData\")\n","                \n","                avg_confidence = total_confidence / processed_count if processed_count > 0 else 0\n","                print(f\"‚úÖ Processed {processed_count} images with MCP\")\n","                print(f\"üìä Average Confidence: {avg_confidence:.3f}\")\n","                \n","                return processed_results\n","\n","        except Exception as e:\n","            print(f\"‚ùå Error in MCP-enhanced processing: {e}\")\n","            return []\n","\n","# Initialize and run MCP-enhanced processor\n","yolo_processor = MCPEnhancedYOLOProcessor(mcp)\n","yolo_results = yolo_processor.process_with_governance()\n","\n","# =============================================================================\n","# STEP 4: RAG Predictions with MCP\n","# =============================================================================\n","print(\"\\nüîÆ STEP 4: RAG PREDICTIONS WITH MCP GOVERNANCE...\")\n","\n","class MCPEnhancedRAGPredictor:\n","    def __init__(self, mcp_system):\n","        self.mcp = mcp_system\n","        self.rag_model_id = \"Traffic_Prediction_RAG_v1.2\"\n","        self.knowledge_base = {}\n","    \n","    def build_knowledge_base(self):\n","        \"\"\"Build knowledge base with quality assessment\"\"\"\n","        print(\"üìö Building MCP-governed knowledge base...\")\n","        \n","        try:\n","            historical_data = spark.sql(\"SELECT * FROM HistoricalTraffic\").toPandas()\n","            \n","            for _, row in historical_data.iterrows():\n","                pattern_key = f\"{row['zone_id']}_{row['hour']:02d}_{row['weather_condition']}\"\n","                \n","                if pattern_key not in self.knowledge_base:\n","                    self.knowledge_base[pattern_key] = []\n","                \n","                pattern_quality = self._assess_pattern_quality(row)\n","                \n","                self.knowledge_base[pattern_key].append({\n","                    'occupancy': row['average_occupancy'],\n","                    'volume': row['traffic_volume'],\n","                    'is_event': row['event_day'],\n","                    'quality_score': pattern_quality,\n","                    'data_hash': row['data_hash']\n","                })\n","            \n","            print(f\"‚úÖ Knowledge base built with {len(self.knowledge_base)} patterns\")\n","            \n","        except Exception as e:\n","            print(f\"‚ùå Error building knowledge base: {e}\")\n","    \n","    def predict_with_governance(self, zone_id, target_hour, weather, is_event=False):\n","        \"\"\"Make predictions with full MCP governance\"\"\"\n","        # Find similar patterns\n","        similar_patterns = self._find_quality_patterns(zone_id, target_hour, weather, is_event)\n","        \n","        if similar_patterns:\n","            # Quality-weighted prediction\n","            total_weight = 0\n","            weighted_occupancy = 0\n","            \n","            for pattern in similar_patterns:\n","                weight = pattern['quality_score']\n","                weighted_occupancy += weight * pattern['occupancy']\n","                total_weight += weight\n","            \n","            predicted_occupancy = weighted_occupancy / total_weight\n","            confidence = min(0.95, total_weight / len(similar_patterns))\n","        else:\n","            predicted_occupancy = 0.5\n","            confidence = 0.3\n","        \n","        # Add realistic noise\n","        noise = random.uniform(-0.05, 0.05)\n","        final_prediction = max(0, min(1, predicted_occupancy + noise))\n","        \n","        # Log prediction with MCP\n","        input_data = {\n","            'zone_id': zone_id,\n","            'target_hour': target_hour,\n","            'weather': weather,\n","            'is_event': is_event\n","        }\n","        \n","        output_data = {\n","            'predicted_occupancy': final_prediction,\n","            'confidence': confidence,\n","            'patterns_used': len(similar_patterns)\n","        }\n","        \n","        receipt = self.mcp.log_inference(\n","            self.rag_model_id,\n","            input_data,\n","            output_data,\n","            confidence\n","        )\n","        \n","        return {\n","            'zone_id': zone_id,\n","            'target_hour': target_hour,\n","            'predicted_occupancy': round(final_prediction, 3),\n","            'confidence': round(confidence, 3),\n","            'weather_condition': weather,\n","            'is_event_day': is_event,\n","            'quality_patterns_used': len(similar_patterns),\n","            'mcp_inference_id': receipt['transaction_id'] if receipt else None,\n","            'prediction_time': datetime.now()\n","        }\n","    \n","    def _assess_pattern_quality(self, data_row):\n","        \"\"\"Assess quality of historical patterns\"\"\"\n","        quality_score = 0.7  # Base score\n","        \n","        # Enhance score based on data characteristics\n","        if data_row['traffic_volume'] > 1000:\n","            quality_score += 0.1  # High volume data is more reliable\n","        \n","        if data_row['event_day']:\n","            quality_score -= 0.1  # Event days might be outliers\n","        \n","        return min(0.95, max(0.5, quality_score))\n","    \n","    def _find_quality_patterns(self, zone_id, target_hour, weather, is_event):\n","        \"\"\"Find high-quality patterns for prediction\"\"\"\n","        pattern_key = f\"{zone_id}_{target_hour:02d}_{weather}\"\n","        base_patterns = self.knowledge_base.get(pattern_key, [])\n","        \n","        # Filter by event day match and quality\n","        quality_patterns = [\n","            p for p in base_patterns \n","            if p['is_event'] == is_event and p['quality_score'] > 0.6\n","        ]\n","        \n","        # Sort by quality and return top patterns\n","        quality_patterns.sort(key=lambda x: x['quality_score'], reverse=True)\n","        return quality_patterns[:5]\n","    \n","    def generate_governed_predictions(self):\n","        \"\"\"Generate predictions with full MCP governance\"\"\"\n","        print(\"üéØ Generating MCP-governed predictions...\")\n","        \n","        self.build_knowledge_base()\n","        \n","        zones = ['ZONE_A', 'ZONE_B', 'ZONE_C', 'ZONE_D', 'ZONE_E']\n","        weather_conditions = ['Sunny', 'Rainy', 'Cloudy']\n","        predictions = []\n","        \n","        for zone in zones:\n","            for hour in range(24):\n","                weather = random.choice(weather_conditions)\n","                is_event = random.random() < 0.1\n","                \n","                prediction = self.predict_with_governance(zone, hour, weather, is_event)\n","                predictions.append(prediction)\n","        \n","        return predictions\n","\n","# Initialize and run MCP-enhanced RAG predictor\n","rag_predictor = MCPEnhancedRAGPredictor(mcp)\n","governed_predictions = rag_predictor.generate_governed_predictions()\n","\n","# Save governed predictions\n","if governed_predictions:\n","    spark.sql(\"DROP TABLE IF EXISTS TrafficPredictions\")\n","    spark.sql(\"\"\"\n","        CREATE TABLE TrafficPredictions (\n","            zone_id STRING,\n","            target_hour INT,\n","            predicted_occupancy DOUBLE,\n","            confidence DOUBLE,\n","            weather_condition STRING,\n","            is_event_day BOOLEAN,\n","            quality_patterns_used INT,\n","            mcp_inference_id STRING,\n","            prediction_time TIMESTAMP\n","        ) USING DELTA\n","    \"\"\")\n","    \n","    predictions_df = spark.createDataFrame(governed_predictions)\n","    predictions_df.write.mode(\"append\").format(\"delta\").saveAsTable(\"TrafficPredictions\")\n","    print(f\"‚úÖ Saved {len(governed_predictions)} MCP-governed predictions\")\n","\n","# =============================================================================\n","# STEP 5: MCP System Performance Analysis\n","# =============================================================================\n","print(\"\\nüìà STEP 5: MCP SYSTEM PERFORMANCE ANALYSIS...\")\n","\n","# Model Registry Analysis\n","try:\n","    model_performance = spark.sql(\"\"\"\n","        SELECT \n","            model_type,\n","            COUNT(*) as model_count,\n","            AVG(CAST(JSON_EXTRACT(performance_metrics, '$.accuracy') AS DOUBLE)) as avg_accuracy,\n","            AVG(CAST(JSON_EXTRACT(performance_metrics, '$.precision') AS DOUBLE)) as avg_precision,\n","            status,\n","            COUNT(DISTINCT blockchain_tx_id) as blockchain_registered\n","        FROM ModelRegistry\n","        GROUP BY model_type, status\n","    \"\"\")\n","    \n","    print(\"‚öñÔ∏è MCP MODEL REGISTRY ANALYSIS:\")\n","    model_performance.show()\n","except Exception as e:\n","    print(f\"‚ùå Error in model registry analysis: {e}\")\n","\n","# Inference Audit Trail\n","try:\n","    inference_stats = spark.sql(\"\"\"\n","        SELECT \n","            COUNT(*) as total_inferences,\n","            COUNT(DISTINCT mcp_inference_id) as blockchain_audited,\n","            AVG(processing_confidence) as avg_confidence\n","        FROM YOLOProcessedData\n","        WHERE mcp_inference_id IS NOT NULL\n","    \"\"\").collect()[0]\n","    \n","    print(f\"\\nüìù MCP INFERENCE AUDIT TRAIL:\")\n","    print(f\"   Total Inferences: {inference_stats['total_inferences']}\")\n","    print(f\"   Blockchain Audited: {inference_stats['blockchain_audited']}\")\n","    print(f\"   Average Confidence: {inference_stats['avg_confidence']:.3f}\")\n","except Exception as e:\n","    print(f\"‚ùå Error in inference audit: {e}\")\n","\n","# =============================================================================\n","# FINAL SUMMARY\n","# =============================================================================\n","print(\"\\n\" + \"=\"*60)\n","print(\"üéâ AI PROCESSING WITH MCP GOVERNANCE COMPLETED!\")\n","print(\"=\"*60)\n","\n","print(\"\\nüìä MCP SYSTEM STATUS:\")\n","mcp_tables = [\"ModelRegistry\", \"YOLOProcessedData\", \"TrafficPredictions\"]\n","for table in mcp_tables:\n","    try:\n","        count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {table}\").collect()[0]['cnt']\n","        blockchain_count = spark.sql(f\"SELECT COUNT(DISTINCT blockchain_tx_id) as cnt FROM {table} WHERE blockchain_tx_id IS NOT NULL\").collect()[0]['cnt']\n","        print(f\"   {table}: {count} records ({blockchain_count} blockchain-verified)\")\n","    except:\n","        print(f\"   {table}: Not available\")\n","\n","print(\"\\n‚úÖ NEXT STEP: Run Notebook 3 for Power BI Dashboard & Advanced Analytics\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"940ad40c-2481-4b20-988f-eb2d6206876e","normalized_state":"finished","queued_time":"2025-10-10T17:58:27.7175098Z","session_start_time":null,"execution_start_time":"2025-10-10T17:58:27.7200665Z","execution_finish_time":"2025-10-10T17:59:31.0779611Z","parent_msg_id":"916c7a5a-999c-4276-9414-c9c5c544745d"},"text/plain":"StatementMeta(, 940ad40c-2481-4b20-988f-eb2d6206876e, 4, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["ü§ñ REIMAGE-AI SMART PARKING - AI PROCESSING WITH MCP\n‚öôÔ∏è Re-initializing Hedera Blockchain Manager...\nüîê Hedera Manager initialized (Test Mode)\n‚öñÔ∏è STEP 1: MODEL CONTEXT PROTOCOL (MCP) IMPLEMENTATION...\n‚úÖ MCP initialized with Hedera integration\n\n\nüìù STEP 2: REGISTERING AI MODELS WITH MCP GOVERNANCE...\nüîß Registering models with full governance...\n‚úÖ Registered model YOLO_Parking_Detection_v2.1\n‚úÖ Registered model Traffic_Prediction_RAG_v1.2\n‚úÖ Registered model Parking_Occupancy_Predictor_v1.1\n\nüöÄ DEPLOYING MODELS...\n‚ö†Ô∏è Registry save error: [_LEGACY_ERROR_TEMP_DELTA_0007] A schema mismatch detected when writing to the Delta table (Table ID: ebfcf1a1-0164-441d-b9a6-865807dfa239).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- blockchain_tx_id: string (nullable = true)\n-- description: string (nullable = true)\n-- last_updated: timestamp (nullable = true)\n-- model_hash: string (nullable = true)\n-- model_id: string (nullable = true)\n-- model_name: string (nullable = true)\n-- model_type: string (nullable = true)\n-- performance_metrics: map (nullable = true)\n    |-- key: string\n    |-- value: double (valueContainsNull = true)\n-- registered_date: timestamp (nullable = true)\n-- status: string (nullable = true)\n-- version: string (nullable = true)\n\n\nData schema:\nroot\n-- blockchain_tx_id: string (nullable = true)\n-- deployed_date: timestamp (nullable = true)\n-- description: string (nullable = true)\n-- last_updated: timestamp (nullable = true)\n-- model_hash: string (nullable = true)\n-- model_id: string (nullable = true)\n-- model_name: string (nullable = true)\n-- model_type: string (nullable = true)\n-- performance_metrics: map (nullable = true)\n    |-- key: string\n    |-- value: double (valueContainsNull = true)\n-- registered_date: timestamp (nullable = true)\n-- status: string (nullable = true)\n-- version: string (nullable = true)\n\n         \nüöÄ Deployed YOLO_Parking_Detection_v2.1 ‚Üí PRODUCTION\n‚ö†Ô∏è Registry save error: [_LEGACY_ERROR_TEMP_DELTA_0007] A schema mismatch detected when writing to the Delta table (Table ID: ebfcf1a1-0164-441d-b9a6-865807dfa239).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- blockchain_tx_id: string (nullable = true)\n-- description: string (nullable = true)\n-- last_updated: timestamp (nullable = true)\n-- model_hash: string (nullable = true)\n-- model_id: string (nullable = true)\n-- model_name: string (nullable = true)\n-- model_type: string (nullable = true)\n-- performance_metrics: map (nullable = true)\n    |-- key: string\n    |-- value: double (valueContainsNull = true)\n-- registered_date: timestamp (nullable = true)\n-- status: string (nullable = true)\n-- version: string (nullable = true)\n\n\nData schema:\nroot\n-- blockchain_tx_id: string (nullable = true)\n-- deployed_date: timestamp (nullable = true)\n-- description: string (nullable = true)\n-- last_updated: timestamp (nullable = true)\n-- model_hash: string (nullable = true)\n-- model_id: string (nullable = true)\n-- model_name: string (nullable = true)\n-- model_type: string (nullable = true)\n-- performance_metrics: map (nullable = true)\n    |-- key: string\n    |-- value: double (valueContainsNull = true)\n-- registered_date: timestamp (nullable = true)\n-- status: string (nullable = true)\n-- version: string (nullable = true)\n\n         \nüöÄ Deployed Traffic_Prediction_RAG_v1.2 ‚Üí STAGING\n‚ö†Ô∏è Registry save error: [_LEGACY_ERROR_TEMP_DELTA_0007] A schema mismatch detected when writing to the Delta table (Table ID: ebfcf1a1-0164-441d-b9a6-865807dfa239).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- blockchain_tx_id: string (nullable = true)\n-- description: string (nullable = true)\n-- last_updated: timestamp (nullable = true)\n-- model_hash: string (nullable = true)\n-- model_id: string (nullable = true)\n-- model_name: string (nullable = true)\n-- model_type: string (nullable = true)\n-- performance_metrics: map (nullable = true)\n    |-- key: string\n    |-- value: double (valueContainsNull = true)\n-- registered_date: timestamp (nullable = true)\n-- status: string (nullable = true)\n-- version: string (nullable = true)\n\n\nData schema:\nroot\n-- blockchain_tx_id: string (nullable = true)\n-- deployed_date: timestamp (nullable = true)\n-- description: string (nullable = true)\n-- last_updated: timestamp (nullable = true)\n-- model_hash: string (nullable = true)\n-- model_id: string (nullable = true)\n-- model_name: string (nullable = true)\n-- model_type: string (nullable = true)\n-- performance_metrics: map (nullable = true)\n    |-- key: string\n    |-- value: double (valueContainsNull = true)\n-- registered_date: timestamp (nullable = true)\n-- status: string (nullable = true)\n-- version: string (nullable = true)\n\n         \nüöÄ Deployed Parking_Occupancy_Predictor_v1.1 ‚Üí DEVELOPMENT\n\nüñºÔ∏è STEP 3: ENHANCED YOLO PROCESSING WITH MCP...\nüéØ Processing traffic images with MCP governance...\n‚ùå Error in MCP-enhanced processing: [DELTA_FAILED_TO_MERGE_FIELDS] Failed to merge fields 'original_vehicle_count' and 'original_vehicle_count'\n\nüîÆ STEP 4: RAG PREDICTIONS WITH MCP GOVERNANCE...\nüéØ Generating MCP-governed predictions...\nüìö Building MCP-governed knowledge base...\n‚úÖ Knowledge base built with 269 patterns\n"]},{"output_type":"error","ename":"AnalysisException","evalue":"[DELTA_FAILED_TO_MERGE_FIELDS] Failed to merge fields 'target_hour' and 'target_hour'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 490\u001b[0m\n\u001b[1;32m    475\u001b[0m     spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;124m        CREATE TABLE TrafficPredictions (\u001b[39m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;124m            zone_id STRING,\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;124m        ) USING DELTA\u001b[39m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[1;32m    489\u001b[0m     predictions_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(governed_predictions)\n\u001b[0;32m--> 490\u001b[0m     predictions_df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msaveAsTable(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrafficPredictions\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Saved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(governed_predictions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m MCP-governed predictions\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    493\u001b[0m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;66;03m# STEP 5: MCP System Performance Analysis\u001b[39;00m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n","File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py:1586\u001b[0m, in \u001b[0;36mDataFrameWriter.saveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1585\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m-> 1586\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msaveAsTable(name)\n","File \u001b[0;32m~/cluster-env/trident_env/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n","\u001b[0;31mAnalysisException\u001b[0m: [DELTA_FAILED_TO_MERGE_FIELDS] Failed to merge fields 'target_hour' and 'target_hour'"]}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ec1385f2-0231-4a86-bac9-523d5bd05b26"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"06b86e1a-668d-4d65-a5f0-7ba8a61fa57d"}],"default_lakehouse":"06b86e1a-668d-4d65-a5f0-7ba8a61fa57d","default_lakehouse_name":"ParkingDataLakehouse","default_lakehouse_workspace_id":"d3afc09a-dc29-418e-be57-836e9d2cc5f1"}}},"nbformat":4,"nbformat_minor":5}
{"cells":[{"cell_type":"code","source":["# COMPLETE SETUP & DATA GENERATION - RUN THIS FIRST\n","# Make sure Lakehouse is attached before running!\n","\n","print(\"ğŸš€ REIMAGE-AI SMART PARKING - COMPLETE SETUP\")\n","\n","# =============================================================================\n","# STEP 1: Verify Lakehouse Connection\n","# =============================================================================\n","print(\"ğŸ”§ STEP 1: VERIFYING LAKEHOUSE CONNECTION...\")\n","\n","try:\n","    tables = spark.sql(\"SHOW TABLES\").collect()\n","    print(f\"âœ… Lakehouse connected! Found {len(tables)} tables\")\n","except Exception as e:\n","    print(\"âŒ No Lakehouse attached! Please manually attach Lakehouse first:\")\n","    print(\"1. Click 'Add' in top-right of notebook\")\n","    print(\"2. Select 'Existing Lakehouse'\")\n","    print(\"3. Choose 'ParkingDataLakehouse'\")\n","    print(\"4. Run this cell again\")\n","    raise e\n","\n","# =============================================================================\n","# STEP 2: Clean Up Existing Tables (If Any)\n","# =============================================================================\n","print(\"\\nğŸ§¹ STEP 2: CLEANING UP EXISTING TABLES...\")\n","\n","tables_to_clean = [\"ParkingSensorData\", \"TrafficCameraData\", \"HistoricalTraffic\", \"SensorMaintenanceData\"]\n","\n","for table_name in tables_to_clean:\n","    try:\n","        spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n","        print(f\"âœ… Dropped table: {table_name}\")\n","    except Exception as e:\n","        print(f\"â„¹ï¸  Could not drop {table_name}: {e}\")\n","\n","# =============================================================================\n","# STEP 3: Create Fresh Tables\n","# =============================================================================\n","print(\"\\nğŸ“‹ STEP 3: CREATING FRESH TABLES...\")\n","\n","tables_sql = {\n","    \"ParkingSensorData\": \"\"\"\n","        CREATE TABLE ParkingSensorData (\n","            sensor_id STRING,\n","            timestamp TIMESTAMP,\n","            occupancy_status BOOLEAN,\n","            vehicle_count INT,\n","            temperature DOUBLE,\n","            vibration_level DOUBLE,\n","            location STRING,\n","            parking_zone STRING,\n","            transaction_hash STRING\n","        ) USING DELTA\n","    \"\"\",\n","    \"TrafficCameraData\": \"\"\"\n","        CREATE TABLE TrafficCameraData (\n","            camera_id STRING,\n","            timestamp TIMESTAMP,\n","            image_url STRING,\n","            vehicle_count INT,\n","            traffic_density DOUBLE,\n","            average_speed INT,\n","            congestion_level STRING,\n","            processed BOOLEAN,\n","            yolo_results STRING\n","        ) USING DELTA\n","    \"\"\",\n","    \"HistoricalTraffic\": \"\"\"\n","        CREATE TABLE HistoricalTraffic (\n","            zone_id STRING,\n","            date DATE,\n","            hour INT,\n","            average_occupancy DOUBLE,\n","            traffic_volume INT,\n","            weather_condition STRING,\n","            event_day BOOLEAN\n","        ) USING DELTA\n","    \"\"\"\n","}\n","\n","for table_name, sql in tables_sql.items():\n","    try:\n","        spark.sql(sql)\n","        print(f\"âœ… Created: {table_name}\")\n","    except Exception as e:\n","        print(f\"âŒ Failed to create {table_name}: {e}\")\n","\n","print(\"ğŸ‰ Tables created successfully!\")\n","\n","# =============================================================================\n","# STEP 4: Generate Synthetic Data\n","# =============================================================================\n","print(\"\\nğŸš€ STEP 4: GENERATING SYNTHETIC DATA...\")\n","\n","from faker import Faker\n","from datetime import datetime, timedelta\n","import random\n","import json\n","\n","class DataGenerator:\n","    def __init__(self):\n","        self.fake = Faker()\n","    \n","    def generate_parking_data(self, num_records=200):\n","        print(f\"ğŸ…¿ï¸ Generating {num_records} parking records...\")\n","        records = []\n","        locations = ['Downtown', 'Shopping Mall', 'Airport', 'Hospital', 'University']\n","        zones = ['ZONE_A', 'ZONE_B', 'ZONE_C', 'ZONE_D', 'ZONE_E']\n","        \n","        for i in range(num_records):\n","            sensor_id = f\"SENSOR_{random.randint(1, 50):03d}\"\n","            timestamp = self.fake.date_time_between(start_date=\"-30d\", end_date=\"now\")\n","            \n","            # Realistic occupancy patterns\n","            hour = timestamp.hour\n","            if 8 <= hour <= 18:  # Business hours\n","                occupancy_prob = 0.7\n","            else:  # Off hours\n","                occupancy_prob = 0.3\n","                \n","            occupied = random.random() < occupancy_prob\n","            \n","            records.append({\n","                'sensor_id': sensor_id,\n","                'timestamp': timestamp,\n","                'occupancy_status': bool(occupied),\n","                'vehicle_count': int(1 if occupied else 0),  # Explicit int conversion\n","                'temperature': float(round(random.uniform(15.0, 35.0), 2)),\n","                'vibration_level': float(round(random.uniform(0.1, 5.0), 2)),\n","                'location': str(random.choice(locations)),\n","                'parking_zone': str(random.choice(zones)),\n","                'transaction_hash': str(self.fake.sha256())\n","            })\n","        \n","        print(f\"âœ… Generated {len(records)} parking records\")\n","        return records\n","    \n","    def generate_traffic_data(self, num_records=100):\n","        print(f\"ğŸš¦ Generating {num_records} traffic records...\")\n","        records = []\n","        \n","        for i in range(num_records):\n","            camera_id = f\"CAM_{random.randint(1, 20):03d}\"\n","            timestamp = self.fake.date_time_between(start_date=\"-30d\", end_date=\"now\")\n","            \n","            # Time-based traffic patterns\n","            hour = timestamp.hour\n","            if 7 <= hour <= 9 or 16 <= hour <= 18:  # Rush hours\n","                vehicles = random.randint(20, 50)\n","                density = round(random.uniform(0.7, 0.95), 2)\n","                speed = random.randint(20, 40)\n","            else:  # Normal hours\n","                vehicles = random.randint(5, 25)\n","                density = round(random.uniform(0.2, 0.6), 2)\n","                speed = random.randint(40, 60)\n","                \n","            congestion = \"HIGH\" if density > 0.7 else \"MEDIUM\" if density > 0.4 else \"LOW\"\n","            \n","            # YOLO simulation data\n","            yolo_data = {\n","                'vehicles_detected': vehicles,\n","                'confidence': round(random.uniform(0.85, 0.98), 2),\n","                'processing_time_ms': random.randint(100, 300)\n","            }\n","            \n","            records.append({\n","                'camera_id': str(camera_id),\n","                'timestamp': timestamp,\n","                'image_url': str(f\"https://trafficcams.com/{camera_id}/{timestamp.strftime('%Y%m%d_%H%M%S')}.jpg\"),\n","                'vehicle_count': int(vehicles),\n","                'traffic_density': float(density),\n","                'average_speed': int(speed),\n","                'congestion_level': str(congestion),\n","                'processed': bool(True),\n","                'yolo_results': str(json.dumps(yolo_data))\n","            })\n","        \n","        print(f\"âœ… Generated {len(records)} traffic records\")\n","        return records\n","    \n","    def generate_historical_data(self, days=30):\n","        print(f\"ğŸ“… Generating {days} days of historical data...\")\n","        records = []\n","        start_date = datetime.now() - timedelta(days=days)\n","        zones = ['ZONE_A', 'ZONE_B', 'ZONE_C', 'ZONE_D', 'ZONE_E']\n","        weather_types = ['Sunny', 'Rainy', 'Cloudy']\n","        \n","        for zone in zones:\n","            for single_date in (start_date + timedelta(days=n) for n in range(days)):\n","                for hour in range(24):\n","                    is_weekend = single_date.weekday() >= 5\n","                    is_holiday = random.random() < 0.05  # 5% chance of holiday\n","                    \n","                    if is_holiday:\n","                        base_occ = random.uniform(0.7, 0.95)\n","                    elif is_weekend:\n","                        base_occ = random.uniform(0.4, 0.8)\n","                    else:\n","                        base_occ = random.uniform(0.3, 0.9)\n","                    \n","                    hour_mult = 1.0 if 8 <= hour <= 18 else 0.4\n","                    weather = random.choice(weather_types)\n","                    weather_impact = 0.8 if weather == 'Rainy' else 1.0\n","                    \n","                    occupancy = round(base_occ * hour_mult * weather_impact, 3)\n","                    volume = int(occupancy * random.randint(500, 2000))\n","                    \n","                    records.append({\n","                        'zone_id': str(zone),\n","                        'date': single_date.date(),\n","                        'hour': int(hour),\n","                        'average_occupancy': float(occupancy),\n","                        'traffic_volume': int(volume),\n","                        'weather_condition': str(weather),\n","                        'event_day': bool(is_holiday)\n","                    })\n","        \n","        print(f\"âœ… Generated {len(records)} historical records\")\n","        return records\n","\n","# Generate all datasets\n","generator = DataGenerator()\n","\n","print(\"ğŸ¯ GENERATING DATASETS...\")\n","parking_data = generator.generate_parking_data(100)  # Reduced for testing\n","traffic_data = generator.generate_traffic_data(50)   # Reduced for testing\n","historical_data = generator.generate_historical_data(7)  # Reduced for testing\n","\n","print(f\"\\nğŸ“Š GENERATION SUMMARY:\")\n","print(f\"   Parking Records: {len(parking_data)}\")\n","print(f\"   Traffic Records: {len(traffic_data)}\")\n","print(f\"   Historical Records: {len(historical_data)}\")\n","\n","# =============================================================================\n","# STEP 5: Load Data to Lakehouse (Fixed Version)\n","# =============================================================================\n","print(\"\\nğŸ’¾ STEP 5: LOADING DATA TO LAKEHOUSE...\")\n","\n","from pyspark.sql.types import (\n","    StructType, StructField, StringType, IntegerType, DoubleType, BooleanType, TimestampType, DateType\n",")\n","\n","# Define schemas explicitly to avoid Delta merge conflicts\n","parking_schema = StructType([\n","    StructField(\"sensor_id\", StringType(), True),\n","    StructField(\"timestamp\", TimestampType(), True),\n","    StructField(\"occupancy_status\", BooleanType(), True),\n","    StructField(\"vehicle_count\", IntegerType(), True),\n","    StructField(\"temperature\", DoubleType(), True),\n","    StructField(\"vibration_level\", DoubleType(), True),\n","    StructField(\"location\", StringType(), True),\n","    StructField(\"parking_zone\", StringType(), True),\n","    StructField(\"transaction_hash\", StringType(), True)\n","])\n","\n","traffic_schema = StructType([\n","    StructField(\"camera_id\", StringType(), True),\n","    StructField(\"timestamp\", TimestampType(), True),\n","    StructField(\"image_url\", StringType(), True),\n","    StructField(\"vehicle_count\", IntegerType(), True),\n","    StructField(\"traffic_density\", DoubleType(), True),\n","    StructField(\"average_speed\", IntegerType(), True),\n","    StructField(\"congestion_level\", StringType(), True),\n","    StructField(\"processed\", BooleanType(), True),\n","    StructField(\"yolo_results\", StringType(), True)\n","])\n","\n","historical_schema = StructType([\n","    StructField(\"zone_id\", StringType(), True),\n","    StructField(\"date\", DateType(), True),\n","    StructField(\"hour\", IntegerType(), True),\n","    StructField(\"average_occupancy\", DoubleType(), True),\n","    StructField(\"traffic_volume\", IntegerType(), True),\n","    StructField(\"weather_condition\", StringType(), True),\n","    StructField(\"event_day\", BooleanType(), True)\n","])\n","\n","print(\"ğŸ…¿ï¸ Loading parking data...\")\n","try:\n","    parking_df = spark.createDataFrame(parking_data, schema=parking_schema)\n","    parking_df.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable(\"ParkingSensorData\")\n","    parking_count = spark.sql(\"SELECT COUNT(*) as cnt FROM ParkingSensorData\").collect()[0]['cnt']\n","    print(f\"   âœ… ParkingSensorData: {parking_count} records\")\n","except Exception as e:\n","    print(f\"âŒ Error loading parking data: {e}\")\n","\n","print(\"ğŸš¦ Loading traffic data...\")\n","try:\n","    traffic_df = spark.createDataFrame(traffic_data, schema=traffic_schema)\n","    traffic_df.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable(\"TrafficCameraData\")\n","    traffic_count = spark.sql(\"SELECT COUNT(*) as cnt FROM TrafficCameraData\").collect()[0]['cnt']\n","    print(f\"   âœ… TrafficCameraData: {traffic_count} records\")\n","except Exception as e:\n","    print(f\"âŒ Error loading traffic data: {e}\")\n","\n","print(\"ğŸ“… Loading historical data...\")\n","try:\n","    historical_df = spark.createDataFrame(historical_data, schema=historical_schema)\n","    historical_df.write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable(\"HistoricalTraffic\")\n","    historical_count = spark.sql(\"SELECT COUNT(*) as cnt FROM HistoricalTraffic\").collect()[0]['cnt']\n","    print(f\"   âœ… HistoricalTraffic: {historical_count} records\")\n","except Exception as e:\n","    print(f\"âŒ Error loading historical data: {e}\")\n","\n","print(\"ğŸ‰ Data loading completed successfully!\")\n","\n","\n","# =============================================================================\n","# STEP 6: Data Quality Validation\n","# =============================================================================\n","print(\"\\nğŸ” STEP 6: DATA QUALITY VALIDATION...\")\n","\n","# Check data statistics\n","validation_queries = {\n","    \"Parking Data Quality\": \"\"\"\n","        SELECT \n","            COUNT(*) as total_records,\n","            AVG(CASE WHEN occupancy_status = true THEN 1.0 ELSE 0.0 END) as occupancy_rate,\n","            COUNT(DISTINCT sensor_id) as unique_sensors,\n","            COUNT(DISTINCT parking_zone) as zones_covered,\n","            MIN(timestamp) as earliest_date,\n","            MAX(timestamp) as latest_date\n","        FROM ParkingSensorData\n","    \"\"\",\n","    \"Traffic Data Quality\": \"\"\"\n","        SELECT \n","            COUNT(*) as total_records,\n","            AVG(traffic_density) as avg_density,\n","            AVG(vehicle_count) as avg_vehicles,\n","            COUNT(DISTINCT camera_id) as unique_cameras,\n","            COUNT(DISTINCT congestion_level) as congestion_levels\n","        FROM TrafficCameraData\n","    \"\"\",\n","    \"Historical Data Quality\": \"\"\"\n","        SELECT \n","            COUNT(*) as total_records,\n","            AVG(average_occupancy) as avg_occupancy,\n","            AVG(traffic_volume) as avg_volume,\n","            COUNT(DISTINCT zone_id) as unique_zones,\n","            MIN(date) as start_date,\n","            MAX(date) as end_date\n","        FROM HistoricalTraffic\n","    \"\"\"\n","}\n","\n","for check_name, query in validation_queries.items():\n","    try:\n","        result = spark.sql(query).collect()[0]\n","        print(f\"\\n{check_name}:\")\n","        for key, value in result.asDict().items():\n","            if isinstance(value, float):\n","                print(f\"   {key}: {value:.3f}\")\n","            else:\n","                print(f\"   {key}: {value}\")\n","    except Exception as e:\n","        print(f\"âŒ Error in {check_name}: {e}\")\n","\n","# =============================================================================\n","# FINAL SUMMARY\n","# =============================================================================\n","print(\"\\n\" + \"=\"*60)\n","print(\"ğŸ‰ REIMAGE-AI SMART PARKING - SETUP COMPLETED!\")\n","print(\"=\"*60)\n","\n","# Show final table status\n","print(\"\\nğŸ“Š FINAL TABLE STATUS:\")\n","try:\n","    tables = spark.sql(\"SHOW TABLES\").collect()\n","    for table in tables:\n","        try:\n","            count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {table['tableName']}\").collect()[0]['cnt']\n","            print(f\"   {table['tableName']}: {count} records\")\n","        except:\n","            print(f\"   {table['tableName']}: [Error counting]\")\n","except Exception as e:\n","    print(f\"âŒ Error listing tables: {e}\")\n","\n","print(\"\\nâœ… NEXT STEPS:\")\n","print(\"   1. Run Notebook 2: AI Processing & Predictions\")\n","print(\"   2. Run Notebook 3: Monitoring & Dashboard\")\n","print(\"   3. Explore data in Lakehouse tables\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"f2d71779-6f3b-4a3f-b23b-b561d943aa3e","normalized_state":"finished","queued_time":"2025-10-10T17:02:32.2171598Z","session_start_time":null,"execution_start_time":"2025-10-10T17:02:32.2189564Z","execution_finish_time":"2025-10-10T17:03:29.504566Z","parent_msg_id":"10546c32-8759-447d-b795-328160d4ca49"},"text/plain":"StatementMeta(, f2d71779-6f3b-4a3f-b23b-b561d943aa3e, 6, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["ğŸš€ REIMAGE-AI SMART PARKING - COMPLETE SETUP\nğŸ”§ STEP 1: VERIFYING LAKEHOUSE CONNECTION...\nâœ… Lakehouse connected! Found 3 tables\n\nğŸ§¹ STEP 2: CLEANING UP EXISTING TABLES...\nâœ… Dropped table: ParkingSensorData\nâœ… Dropped table: TrafficCameraData\nâœ… Dropped table: HistoricalTraffic\nâœ… Dropped table: SensorMaintenanceData\n\nğŸ“‹ STEP 3: CREATING FRESH TABLES...\nâœ… Created: ParkingSensorData\nâœ… Created: TrafficCameraData\nâœ… Created: HistoricalTraffic\nğŸ‰ Tables created successfully!\n\nğŸš€ STEP 4: GENERATING SYNTHETIC DATA...\nğŸ¯ GENERATING DATASETS...\nğŸ…¿ï¸ Generating 100 parking records...\nâœ… Generated 100 parking records\nğŸš¦ Generating 50 traffic records...\nâœ… Generated 50 traffic records\nğŸ“… Generating 7 days of historical data...\nâœ… Generated 840 historical records\n\nğŸ“Š GENERATION SUMMARY:\n   Parking Records: 100\n   Traffic Records: 50\n   Historical Records: 840\n\nğŸ’¾ STEP 5: LOADING DATA TO LAKEHOUSE...\nğŸ…¿ï¸ Loading parking data...\n   âœ… ParkingSensorData: 100 records\nğŸš¦ Loading traffic data...\n   âœ… TrafficCameraData: 50 records\nğŸ“… Loading historical data...\n   âœ… HistoricalTraffic: 840 records\nğŸ‰ Data loading completed successfully!\n\nğŸ” STEP 6: DATA QUALITY VALIDATION...\n\nParking Data Quality:\n   total_records: 100\n   occupancy_rate: 0.35000\n   unique_sensors: 46\n   zones_covered: 5\n   earliest_date: 2025-09-10 22:03:51.181197\n   latest_date: 2025-10-10 11:34:04.253462\n\nTraffic Data Quality:\n   total_records: 50\n   avg_density: 0.502\n   avg_vehicles: 18.240\n   unique_cameras: 17\n   congestion_levels: 3\n\nHistorical Data Quality:\n   total_records: 840\n   avg_occupancy: 0.388\n   avg_volume: 480.318\n   unique_zones: 5\n   start_date: 2025-10-03\n   end_date: 2025-10-09\n\n============================================================\nğŸ‰ REIMAGE-AI SMART PARKING - SETUP COMPLETED!\n============================================================\n\nğŸ“Š FINAL TABLE STATUS:\n   historicaltraffic: 840 records\n   parkingsensordata: 100 records\n   trafficcameradata: 50 records\n\nâœ… NEXT STEPS:\n   1. Run Notebook 2: AI Processing & Predictions\n   2. Run Notebook 3: Monitoring & Dashboard\n   3. Explore data in Lakehouse tables\n"]}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b4ee0930-071d-4f83-84ae-5222d4664ba0"},{"cell_type":"markdown","source":["#### ENHANCED IMPLEMENTATION WITH HEDERA, MCP & POWER BI"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"06371be1-cbe0-4d1d-8f53-3159269f00e3"},{"cell_type":"code","source":["# =============================================================\n","# ğŸš€ REIMAGE-AI SMART PARKING - WITH HEDERA BLOCKCHAIN\n","# =============================================================\n","\n","print(\"ğŸš€ REIMAGE-AI SMART PARKING - WITH HEDERA BLOCKCHAIN\")\n","\n","# =============================================================\n","# STEP 1: Verify Lakehouse Connection\n","# =============================================================\n","print(\"ğŸ”§ STEP 1: VERIFYING LAKEHOUSE CONNECTION...\")\n","\n","try:\n","    tables = spark.sql(\"SHOW TABLES\").collect()\n","    print(f\"âœ… Lakehouse connected! Found {len(tables)} tables\")\n","except Exception as e:\n","    print(\"âŒ No Lakehouse attached! Please attach a Lakehouse manually before running.\")\n","    raise e\n","\n","\n","# =============================================================\n","# STEP 2: Install Required Packages\n","# =============================================================\n","print(\"\\nğŸ“¦ STEP 2: INSTALLING REQUIRED PACKAGES...\")\n","\n","try:\n","    %pip install faker hedera-sdk-py python-dotenv cryptography\n","    print(\"âœ… Packages installed successfully\")\n","except Exception as e:\n","    print(f\"âš ï¸  Package installation note: {e}\")\n","\n","\n","# =============================================================\n","# STEP 3: Initialize Hedera Blockchain Manager\n","# =============================================================\n","print(\"\\nâ›“ï¸ STEP 3: INITIALIZING HEDERA BLOCKCHAIN INTEGRATION...\")\n","\n","import hashlib\n","import json\n","from datetime import datetime, date, timedelta\n","import random\n","from faker import Faker\n","from pyspark.sql.types import *\n","\n","# ---- Custom JSON encoder for datetime ----\n","class EnhancedJSONEncoder(json.JSONEncoder):\n","    def default(self, obj):\n","        if isinstance(obj, (datetime, date)):\n","            return obj.isoformat()\n","        return super().default(obj)\n","\n","class HederaBlockchainManager:\n","    def __init__(self):\n","        self.hedera_config = {\n","            \"testnet_account_id\": \"0.0.12345\",\n","            \"testnet_private_key\": \"302e...\",\n","            \"topic_id\": None\n","        }\n","        print(\"ğŸ” Hedera Manager Initialized (Test Mode)\")\n","\n","    def calculate_data_hash(self, data):\n","        data_str = json.dumps(data, cls=EnhancedJSONEncoder, sort_keys=True)\n","        return hashlib.sha256(data_str.encode()).hexdigest()\n","\n","    def simulate_hedera_transaction(self, data, transaction_type=\"DATA_STORAGE\"):\n","        data_hash = self.calculate_data_hash(data)\n","        timestamp = datetime.now().isoformat()\n","        receipt = {\n","            \"transaction_id\": f\"0.0.{int(datetime.now().timestamp())}\",\n","            \"data_hash\": data_hash,\n","            \"timestamp\": timestamp,\n","            \"transaction_type\": transaction_type,\n","            \"status\": \"SUCCESS\",\n","            \"consensus_timestamp\": timestamp,\n","            \"topic_id\": \"0.0.456789\",\n","            \"blockchain_verified\": True\n","        }\n","        print(f\"ğŸ”— Simulated Hedera Transaction {receipt['transaction_id']}\")\n","        return receipt\n","\n","    def store_on_blockchain(self, data, metadata=None):\n","        try:\n","            transaction_data = {\n","                \"data\": data,\n","                \"metadata\": metadata or {},\n","                \"storage_timestamp\": datetime.now().isoformat(),\n","                \"data_type\": \"parking_sensor\" if \"sensor_id\" in data else \"traffic_data\"\n","            }\n","            receipt = self.simulate_hedera_transaction(transaction_data)\n","            data_with_proof = data.copy()\n","            data_with_proof.update({\n","                \"blockchain_tx_id\": receipt[\"transaction_id\"],\n","                \"data_hash\": receipt[\"data_hash\"],\n","                \"blockchain_verified\": receipt[\"blockchain_verified\"]\n","            })\n","            return data_with_proof, receipt\n","        except Exception as e:\n","            print(f\"âŒ Blockchain storage error: {e}\")\n","            return data, None\n","\n","hedera_manager = HederaBlockchainManager()\n","\n","\n","# =============================================================\n","# STEP 4: Create Enhanced Tables with Blockchain Fields\n","# =============================================================\n","print(\"\\nğŸ“‹ STEP 4: CREATING ENHANCED TABLES WITH BLOCKCHAIN FIELDS...\")\n","\n","tables_to_clean = [\n","    \"ParkingSensorData\", \"TrafficCameraData\", \"HistoricalTraffic\",\n","    \"BlockchainTransactions\", \"ModelRegistry\", \"PowerBI_Metrics\"\n","]\n","for t in tables_to_clean:\n","    try:\n","        spark.sql(f\"DROP TABLE IF EXISTS {t}\")\n","        print(f\"âœ… Dropped table: {t}\")\n","    except:\n","        print(f\"â„¹ï¸  Could not drop {t}\")\n","\n","tables_sql = {\n","    \"ParkingSensorData\": \"\"\"\n","        CREATE TABLE ParkingSensorData (\n","            sensor_id STRING,\n","            timestamp TIMESTAMP,\n","            occupancy_status BOOLEAN,\n","            vehicle_count INT,\n","            temperature DOUBLE,\n","            vibration_level DOUBLE,\n","            location STRING,\n","            parking_zone STRING,\n","            transaction_hash STRING,\n","            blockchain_tx_id STRING,\n","            data_hash STRING,\n","            blockchain_verified BOOLEAN\n","        ) USING DELTA\n","    \"\"\",\n","    \"TrafficCameraData\": \"\"\"\n","        CREATE TABLE TrafficCameraData (\n","            camera_id STRING,\n","            timestamp TIMESTAMP,\n","            image_url STRING,\n","            vehicle_count INT,\n","            traffic_density DOUBLE,\n","            average_speed INT,\n","            congestion_level STRING,\n","            processed BOOLEAN,\n","            yolo_results STRING,\n","            blockchain_tx_id STRING,\n","            data_hash STRING,\n","            blockchain_verified BOOLEAN\n","        ) USING DELTA\n","    \"\"\",\n","    \"HistoricalTraffic\": \"\"\"\n","        CREATE TABLE HistoricalTraffic (\n","            zone_id STRING,\n","            date DATE,\n","            hour INT,\n","            average_occupancy DOUBLE,\n","            traffic_volume INT,\n","            weather_condition STRING,\n","            event_day BOOLEAN,\n","            data_hash STRING\n","        ) USING DELTA\n","    \"\"\",\n","    \"PowerBI_Metrics\": \"\"\"\n","        CREATE TABLE PowerBI_Metrics (\n","            metric_id STRING,\n","            metric_name STRING,\n","            metric_value DOUBLE,\n","            metric_timestamp TIMESTAMP,\n","            category STRING,\n","            zone_id STRING,\n","            data_source STRING\n","        ) USING DELTA\n","    \"\"\"\n","}\n","\n","for t, sql_stmt in tables_sql.items():\n","    spark.sql(sql_stmt)\n","    print(f\"âœ… Created: {t}\")\n","\n","print(\"ğŸ‰ Enhanced tables created successfully!\")\n","\n","\n","# =============================================================\n","# STEP 5: Generate Synthetic Data with Blockchain Integration\n","# =============================================================\n","print(\"\\nğŸš€ STEP 5: GENERATING SYNTHETIC DATA WITH BLOCKCHAIN...\")\n","\n","fake = Faker()\n","\n","def generate_parking_data(num=100):\n","    records = []\n","    locations = [\"Downtown\", \"Airport\", \"Mall\", \"Hospital\"]\n","    zones = [\"ZONE_A\", \"ZONE_B\", \"ZONE_C\", \"ZONE_D\"]\n","    for _ in range(num):\n","        ts = fake.date_time_between(start_date=\"-30d\", end_date=\"now\")\n","        occupied = random.random() < (0.7 if 8 <= ts.hour <= 18 else 0.3)\n","        base = {\n","            \"sensor_id\": f\"SENSOR_{random.randint(1,50):03d}\",\n","            \"timestamp\": ts,\n","            \"occupancy_status\": occupied,\n","            \"vehicle_count\": 1 if occupied else 0,\n","            \"temperature\": round(random.uniform(15,35),2),\n","            \"vibration_level\": round(random.uniform(0.1,5.0),2),\n","            \"location\": random.choice(locations),\n","            \"parking_zone\": random.choice(zones),\n","            \"transaction_hash\": fake.sha256()\n","        }\n","        enhanced, receipt = hedera_manager.store_on_blockchain(base)\n","        records.append(enhanced)\n","    print(f\"âœ… Generated {len(records)} blockchain-backed parking records\")\n","    return records\n","\n","def generate_traffic_data(num=50):\n","    records = []\n","    for _ in range(num):\n","        ts = fake.date_time_between(start_date=\"-30d\", end_date=\"now\")\n","        rush = (7 <= ts.hour <= 9) or (16 <= ts.hour <= 18)\n","        vehicles = random.randint(20,50) if rush else random.randint(5,25)\n","        density = round(random.uniform(0.7,0.95),2) if rush else round(random.uniform(0.2,0.6),2)\n","        speed = random.randint(20,40) if rush else random.randint(40,60)\n","        congestion = \"HIGH\" if density>0.7 else \"MEDIUM\" if density>0.4 else \"LOW\"\n","        base = {\n","            \"camera_id\": f\"CAM_{random.randint(1,20):03d}\",\n","            \"timestamp\": ts,\n","            \"image_url\": f\"https://trafficcams.com/{ts.strftime('%Y%m%d_%H%M%S')}.jpg\",\n","            \"vehicle_count\": vehicles,\n","            \"traffic_density\": density,\n","            \"average_speed\": speed,\n","            \"congestion_level\": congestion,\n","            \"processed\": True,\n","            \"yolo_results\": json.dumps({\n","                \"vehicles_detected\": vehicles,\n","                \"confidence\": round(random.uniform(0.85,0.98),2)\n","            })\n","        }\n","        enhanced, receipt = hedera_manager.store_on_blockchain(base)\n","        records.append(enhanced)\n","    print(f\"âœ… Generated {len(records)} blockchain-backed traffic records\")\n","    return records\n","\n","def generate_historical_data(days=7):\n","    records=[]\n","    start=datetime.now()-timedelta(days=days)\n","    zones=[\"ZONE_A\",\"ZONE_B\",\"ZONE_C\",\"ZONE_D\"]\n","    weathers=[\"Sunny\",\"Rainy\",\"Cloudy\"]\n","    for zone in zones:\n","        for d in range(days):\n","            date_=start+timedelta(days=d)\n","            for hour in range(24):\n","                occ=round(random.uniform(0.3,0.95)*(1 if 8<=hour<=18 else 0.4),3)\n","                vol=int(occ*random.randint(500,2000))\n","                weather=random.choice(weathers)\n","                record={\n","                    \"zone_id\":zone,\n","                    \"date\":date_.date(),\n","                    \"hour\":hour,\n","                    \"average_occupancy\":occ,\n","                    \"traffic_volume\":vol,\n","                    \"weather_condition\":weather,\n","                    \"event_day\":random.random()<0.05,\n","                    \"data_hash\":hedera_manager.calculate_data_hash({\"z\":zone,\"d\":str(date_.date()),\"h\":hour})\n","                }\n","                records.append(record)\n","    print(f\"âœ… Generated {len(records)} historical records\")\n","    return records\n","\n","parking_data=generate_parking_data()\n","traffic_data=generate_traffic_data()\n","historical_data=generate_historical_data()\n","\n","\n","# =============================================================\n","# STEP 6: Load Data into Lakehouse\n","# =============================================================\n","print(\"\\nğŸ’¾ STEP 6: LOADING ENHANCED DATA TO LAKEHOUSE...\")\n","\n","parking_schema=StructType([\n","    StructField(\"sensor_id\",StringType()),StructField(\"timestamp\",TimestampType()),\n","    StructField(\"occupancy_status\",BooleanType()),StructField(\"vehicle_count\",IntegerType()),\n","    StructField(\"temperature\",DoubleType()),StructField(\"vibration_level\",DoubleType()),\n","    StructField(\"location\",StringType()),StructField(\"parking_zone\",StringType()),\n","    StructField(\"transaction_hash\",StringType()),StructField(\"blockchain_tx_id\",StringType()),\n","    StructField(\"data_hash\",StringType()),StructField(\"blockchain_verified\",BooleanType())\n","])\n","traffic_schema=StructType([\n","    StructField(\"camera_id\",StringType()),StructField(\"timestamp\",TimestampType()),\n","    StructField(\"image_url\",StringType()),StructField(\"vehicle_count\",IntegerType()),\n","    StructField(\"traffic_density\",DoubleType()),StructField(\"average_speed\",IntegerType()),\n","    StructField(\"congestion_level\",StringType()),StructField(\"processed\",BooleanType()),\n","    StructField(\"yolo_results\",StringType()),StructField(\"blockchain_tx_id\",StringType()),\n","    StructField(\"data_hash\",StringType()),StructField(\"blockchain_verified\",BooleanType())\n","])\n","hist_schema=StructType([\n","    StructField(\"zone_id\",StringType()),StructField(\"date\",DateType()),\n","    StructField(\"hour\",IntegerType()),StructField(\"average_occupancy\",DoubleType()),\n","    StructField(\"traffic_volume\",IntegerType()),StructField(\"weather_condition\",StringType()),\n","    StructField(\"event_day\",BooleanType()),StructField(\"data_hash\",StringType())\n","])\n","\n","spark.createDataFrame(parking_data,parking_schema)\\\n","     .write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\",\"true\").saveAsTable(\"ParkingSensorData\")\n","spark.createDataFrame(traffic_data,traffic_schema)\\\n","     .write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\",\"true\").saveAsTable(\"TrafficCameraData\")\n","spark.createDataFrame(historical_data,hist_schema)\\\n","     .write.mode(\"overwrite\").format(\"delta\").option(\"overwriteSchema\",\"true\").saveAsTable(\"HistoricalTraffic\")\n","\n","print(\"ğŸ‰ Enhanced data loading completed!\")\n","\n","\n","# =============================================================\n","# STEP 7: Generate Power BI Metrics\n","# =============================================================\n","print(\"\\nğŸ“Š STEP 7: GENERATING POWER BI METRICS...\")\n","\n","def safe_float(v): return float(v) if v else 0.0\n","\n","def generate_powerbi_metrics():\n","    metrics=[]\n","    now=datetime.now()\n","    try:\n","        blk=spark.sql(\"\"\"\n","            SELECT COUNT(*) as total,\n","                   SUM(CASE WHEN blockchain_verified THEN 1 ELSE 0 END) as verified,\n","                   AVG(CASE WHEN blockchain_verified THEN 1.0 ELSE 0.0 END) as rate\n","            FROM ParkingSensorData\n","        \"\"\").collect()[0]\n","        metrics.append({\"metric_id\":\"blk_total\",\"metric_name\":\"Total Blockchain Records\",\n","                        \"metric_value\":safe_float(blk[\"total\"]),\"metric_timestamp\":now,\n","                        \"category\":\"Blockchain\",\"zone_id\":\"ALL\",\"data_source\":\"Hedera\"})\n","        metrics.append({\"metric_id\":\"blk_rate\",\"metric_name\":\"Blockchain Verification Rate\",\n","                        \"metric_value\":safe_float(blk[\"rate\"]),\"metric_timestamp\":now,\n","                        \"category\":\"Blockchain\",\"zone_id\":\"ALL\",\"data_source\":\"Hedera\"})\n","    except Exception as e:\n","        print(f\"âŒ Metric generation error: {e}\")\n","    return metrics\n","\n","metrics=generate_powerbi_metrics()\n","if metrics:\n","    spark.createDataFrame(metrics).write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"PowerBI_Metrics\")\n","    print(f\"âœ… Generated {len(metrics)} Power BI metrics\")\n","\n","# =============================================================\n","# FINAL SUMMARY\n","# =============================================================\n","print(\"\\n\" + \"=\"*60)\n","print(\"ğŸ‰ REIMAGE-AI SMART PARKING WITH HEDERA - SETUP COMPLETED!\")\n","print(\"=\"*60)\n","\n","for t in [\"ParkingSensorData\",\"TrafficCameraData\",\"HistoricalTraffic\",\"PowerBI_Metrics\"]:\n","    c=spark.sql(f\"SELECT COUNT(*) as cnt FROM {t}\").collect()[0]['cnt']\n","    print(f\"   {t}: {c} records\")\n","\n","print(\"\\nâœ… NEXT STEPS:\")\n","print(\"   1. Run Notebook 2 - AI Processing & MCP Integration\")\n","print(\"   2. Run Notebook 3 - Power BI Dashboard & Monitoring\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":23,"statement_ids":[18,19,20,21,22,23],"state":"finished","livy_statement_state":"available","session_id":"d0b4adbe-9011-4d4d-acd3-20483e4fe494","normalized_state":"finished","queued_time":"2025-10-10T17:51:46.5732529Z","session_start_time":null,"execution_start_time":"2025-10-10T17:51:47.8640481Z","execution_finish_time":"2025-10-10T17:52:46.6809535Z","parent_msg_id":"6a9750fd-259d-4b29-a5ba-a933b92c143a"},"text/plain":"StatementMeta(, d0b4adbe-9011-4d4d-acd3-20483e4fe494, 23, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: faker in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (37.11.0)\nRequirement already satisfied: hedera-sdk-py in /nfs4/pyenv-0ae02918-bb11-432b-86b5-dbae1e3dcb87/lib/python3.11/site-packages (2.50.0)\nRequirement already satisfied: python-dotenv in /nfs4/pyenv-0ae02918-bb11-432b-86b5-dbae1e3dcb87/lib/python3.11/site-packages (1.1.1)\nRequirement already satisfied: cryptography in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (42.0.2)\nRequirement already satisfied: tzdata in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from faker) (2023.3)\nRequirement already satisfied: pyjnius>=1.6.1 in /nfs4/pyenv-0ae02918-bb11-432b-86b5-dbae1e3dcb87/lib/python3.11/site-packages (from hedera-sdk-py) (1.7.0)\nRequirement already satisfied: cffi>=1.12 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from cryptography) (1.16.0)\nRequirement already satisfied: pycparser in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from cffi>=1.12->cryptography) (2.21)\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\nğŸš€ REIMAGE-AI SMART PARKING - WITH HEDERA BLOCKCHAIN\nğŸ”§ STEP 1: VERIFYING LAKEHOUSE CONNECTION...\nâœ… Lakehouse connected! Found 11 tables\n\nğŸ“¦ STEP 2: INSTALLING REQUIRED PACKAGES...\nâœ… Packages installed successfully\n\nâ›“ï¸ STEP 3: INITIALIZING HEDERA BLOCKCHAIN INTEGRATION...\nğŸ” Hedera Manager Initialized (Test Mode)\n\nğŸ“‹ STEP 4: CREATING ENHANCED TABLES WITH BLOCKCHAIN FIELDS...\nâœ… Dropped table: ParkingSensorData\nâœ… Dropped table: TrafficCameraData\nâœ… Dropped table: HistoricalTraffic\nâœ… Dropped table: BlockchainTransactions\nâœ… Dropped table: ModelRegistry\nâœ… Dropped table: PowerBI_Metrics\nâœ… Created: ParkingSensorData\nâœ… Created: TrafficCameraData\nâœ… Created: HistoricalTraffic\nâœ… Created: PowerBI_Metrics\nğŸ‰ Enhanced tables created successfully!\n\nğŸš€ STEP 5: GENERATING SYNTHETIC DATA WITH BLOCKCHAIN...\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nâœ… Generated 100 blockchain-backed parking records\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nğŸ”— Simulated Hedera Transaction 0.0.1760118728\nâœ… Generated 50 blockchain-backed traffic records\nâœ… Generated 672 historical records\n\nğŸ’¾ STEP 6: LOADING ENHANCED DATA TO LAKEHOUSE...\nğŸ‰ Enhanced data loading completed!\n\nğŸ“Š STEP 7: GENERATING POWER BI METRICS...\nâœ… Generated 2 Power BI metrics\n\n============================================================\nğŸ‰ REIMAGE-AI SMART PARKING WITH HEDERA - SETUP COMPLETED!\n============================================================\n   ParkingSensorData: 100 records\n   TrafficCameraData: 50 records\n   HistoricalTraffic: 672 records\n   PowerBI_Metrics: 2 records\n\nâœ… NEXT STEPS:\n   1. Run Notebook 2 - AI Processing & MCP Integration\n   2. Run Notebook 3 - Power BI Dashboard & Monitoring\nWarning: PySpark kernel has been restarted to use updated packages.\n\n"]}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5d1bed84-80f5-48af-9344-0fe3161ab9e6"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"06b86e1a-668d-4d65-a5f0-7ba8a61fa57d"}],"default_lakehouse":"06b86e1a-668d-4d65-a5f0-7ba8a61fa57d","default_lakehouse_name":"ParkingDataLakehouse","default_lakehouse_workspace_id":"d3afc09a-dc29-418e-be57-836e9d2cc5f1"}}},"nbformat":4,"nbformat_minor":5}
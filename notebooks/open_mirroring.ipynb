{"cells":[{"cell_type":"code","source":["# =============================================================\n","# üîÅ OPEN MIRRORING CONFIGURATION (Auto-Healing Simulation Mode)\n","# =============================================================\n","\n","from pyspark.sql import SparkSession\n","from pyspark.sql.types import StructType, StructField, StringType\n","from pyspark.sql import functions as F\n","\n","class OpenMirroringManager:\n","    \"\"\"\n","    Handles configuration of Open Mirroring in Microsoft Fabric Lakehouse.\n","    Falls back to simulation mode outside Fabric, creating demo data if needed.\n","    \"\"\"\n","\n","    def __init__(self, spark: SparkSession, database_name=\"ParkingDataLakehouse\"):\n","        self.spark = spark\n","        self.database_name = database_name\n","\n","    # ---------------------------------------------------------\n","    # Step 1 ‚Äì Enable Fabric Mirroring (if supported)\n","    # ---------------------------------------------------------\n","    def configure_open_mirroring(self):\n","        print(\"üöÄ STARTING OPEN MIRRORING SETUP\")\n","        print(\"üîÑ CONFIGURING OPEN MIRRORING FOR DELTA REPLICATION...\")\n","\n","        try:\n","            cluster_info = self.spark.conf.get(\n","                \"spark.databricks.clusterUsageTags.clusterType\", \"\"\n","            ).lower()\n","\n","            if \"fabric\" in cluster_info or \"synapse\" in cluster_info:\n","                print(\"üß† Detected Microsoft Fabric runtime.\")\n","                self.spark.sql(f\"\"\"\n","                    ALTER DATABASE {self.database_name}\n","                    SET DBPROPERTIES ('enable_mirroring'='true')\n","                \"\"\")\n","                print(f\"‚úÖ Mirroring enabled for {self.database_name}\")\n","            else:\n","                print(\"üß© Non-Fabric runtime detected ‚Äî switching to simulation mode.\")\n","                print(\"‚ÑπÔ∏è Skipping ALTER DATABASE (Fabric-only feature).\")\n","        except Exception as e:\n","            print(f\"‚ö†Ô∏è Mirroring note: {e}\")\n","            print(\"‚öôÔ∏è Continuing in simulation mode...\")\n","\n","    # ---------------------------------------------------------\n","    # Step 2 ‚Äì Ensure mock tables exist (create if missing)\n","    # ---------------------------------------------------------\n","    def _ensure_table_exists(self, table_name: str):\n","        full_name = f\"{self.database_name}.{table_name}\"\n","        try:\n","            self.spark.table(full_name)\n","            print(f\"‚úÖ Found source table: {full_name}\")\n","            return True\n","        except Exception:\n","            print(f\"‚ÑπÔ∏è Table not found: {full_name} ‚Äî creating mock data...\")\n","\n","            # Create mock DataFrame per table\n","            if table_name == \"ParkingSensorData\":\n","                df = self.spark.createDataFrame([\n","                    (\"Zone A\", True, \"2025-10-11 08:00\", True, \"tx001\"),\n","                    (\"Zone B\", False, \"2025-10-11 08:05\", True, \"tx002\"),\n","                ], [\"parking_zone\", \"occupancy_status\", \"timestamp\",\n","                    \"blockchain_verified\", \"blockchain_tx_id\"])\n","\n","            elif table_name == \"TrafficCameraData\":\n","                df = self.spark.createDataFrame([\n","                    (\"Cam-1\", \"HIGH\", 0.85, 34, \"2025-10-11 08:00\"),\n","                    (\"Cam-2\", \"MEDIUM\", 0.50, 20, \"2025-10-11 08:05\"),\n","                ], [\"camera_id\", \"congestion_level\", \"traffic_density\",\n","                    \"vehicle_count\", \"timestamp\"])\n","\n","            elif table_name == \"YOLOProcessedData\":\n","                df = self.spark.createDataFrame([\n","                    (\"Cam-1\", 0.92, \"SUCCESS\", \"2025-10-11 08:00\"),\n","                    (\"Cam-2\", 0.78, \"SUCCESS\", \"2025-10-11 08:05\"),\n","                ], [\"camera_id\", \"processing_confidence\",\n","                    \"processing_status\", \"processed_at\"])\n","            else:\n","                schema = StructType([StructField(\"id\", StringType())])\n","                df = self.spark.createDataFrame([], schema)\n","\n","            # Ensure database exists\n","            self.spark.sql(f\"CREATE DATABASE IF NOT EXISTS {self.database_name}\")\n","            df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(full_name)\n","            print(f\"‚úÖ Mock table created: {full_name}\")\n","            return True\n","\n","    # ---------------------------------------------------------\n","    # Step 3 ‚Äì Simulate Mirroring by copying to local schema\n","    # ---------------------------------------------------------\n","    def simulate_replication(self, table_names=None):\n","        if table_names is None:\n","            table_names = [\"ParkingSensorData\",\n","                           \"TrafficCameraData\",\n","                           \"YOLOProcessedData\"]\n","\n","        print(\"\\nüß± SIMULATING OPEN MIRRORING (Local Copy Mode)...\")\n","        self.spark.sql(\"CREATE DATABASE IF NOT EXISTS MirroredData\")\n","\n","        for table in table_names:\n","            src_table = f\"{self.database_name}.{table}\"\n","            dst_table = f\"MirroredData.{table}\"\n","\n","            self._ensure_table_exists(table)\n","\n","            try:\n","                df = self.spark.table(src_table)\n","                df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(dst_table)\n","                print(f\"‚úÖ Mirrored table created: {dst_table} ({df.count()} rows)\")\n","            except Exception as e:\n","                print(f\"‚ö†Ô∏è Skipping {table}: {e}\")\n","\n","        print(\"üèÅ Mirroring simulation completed successfully.\\n\")\n","\n","# ---------------------------------------------------------\n","# ‚úÖ Optional standalone execution\n","# ---------------------------------------------------------\n","if __name__ == \"__main__\":\n","    spark = SparkSession.builder.appName(\"OpenMirroringSetup\").getOrCreate()\n","    mgr = OpenMirroringManager(spark)\n","    mgr.configure_open_mirroring()\n","    mgr.simulate_replication()\n","    print(\"‚úÖ Open Mirroring setup completed successfully.\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"36585711-9b79-4c83-8c72-643e2aa1302f","normalized_state":"finished","queued_time":"2025-10-17T16:24:51.6021152Z","session_start_time":"2025-10-17T16:24:51.6033984Z","execution_start_time":"2025-10-17T16:25:05.2394374Z","execution_finish_time":"2025-10-17T16:25:57.364791Z","parent_msg_id":"fd2bef98-b157-4e44-8e89-3b414b6683a8"},"text/plain":"StatementMeta(, 36585711-9b79-4c83-8c72-643e2aa1302f, 3, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["üöÄ STARTING OPEN MIRRORING SETUP\nüîÑ CONFIGURING OPEN MIRRORING FOR DELTA REPLICATION...\nüß© Non-Fabric runtime detected ‚Äî switching to simulation mode.\n‚ÑπÔ∏è Skipping ALTER DATABASE (Fabric-only feature).\n\nüß± SIMULATING OPEN MIRRORING (Local Copy Mode)...\n‚úÖ Found source table: ParkingDataLakehouse.ParkingSensorData\n‚úÖ Mirrored table created: MirroredData.ParkingSensorData (2 rows)\n‚úÖ Found source table: ParkingDataLakehouse.TrafficCameraData\n‚úÖ Mirrored table created: MirroredData.TrafficCameraData (2 rows)\n‚úÖ Found source table: ParkingDataLakehouse.YOLOProcessedData\n‚úÖ Mirrored table created: MirroredData.YOLOProcessedData (2 rows)\nüèÅ Mirroring simulation completed successfully.\n\n‚úÖ Open Mirroring setup completed successfully.\n"]}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"47c76208-7d6e-44e9-a7c9-c72116612aa6"},{"cell_type":"code","source":["# =============================================================\n","# üîÅ OPEN MIRRORING CONFIGURATION + SAFE EXPORT (Auto-Healing Mode)\n","# =============================================================\n","from pyspark.sql import SparkSession\n","from pyspark.sql.types import StructType, StructField, StringType\n","from pyspark.sql import functions as F\n","import os, shutil, pandas as pd\n","\n","class OpenMirroringManager:\n","    \"\"\"\n","    Handles configuration of Open Mirroring in Microsoft Fabric Lakehouse.\n","    Falls back to simulation mode outside Fabric, creating demo data if needed.\n","    Now includes automatic Safe Exporter for CSV download.\n","    \"\"\"\n","\n","    def __init__(self, spark: SparkSession, database_name=\"ParkingDataLakehouse\", mirror_name=\"MirroredData\"):\n","        self.spark = spark\n","        self.database_name = database_name\n","        self.mirror_name = mirror_name\n","        self.output_dir = \"/tmp/lakehouse_export_local\"\n","        os.makedirs(self.output_dir, exist_ok=True)\n","\n","    # ---------------------------------------------------------\n","    # Step 1 ‚Äì Enable Fabric Mirroring (if supported)\n","    # ---------------------------------------------------------\n","    def configure_open_mirroring(self):\n","        print(\"üöÄ STARTING OPEN MIRRORING SETUP\")\n","        print(\"üîÑ CONFIGURING OPEN MIRRORING FOR DELTA REPLICATION...\")\n","\n","        try:\n","            cluster_info = self.spark.conf.get(\n","                \"spark.databricks.clusterUsageTags.clusterType\", \"\"\n","            ).lower()\n","\n","            if \"fabric\" in cluster_info or \"synapse\" in cluster_info:\n","                print(\"üß† Detected Microsoft Fabric runtime.\")\n","                self.spark.sql(f\"\"\"\n","                    ALTER DATABASE {self.database_name}\n","                    SET DBPROPERTIES ('enable_mirroring'='true')\n","                \"\"\")\n","                print(f\"‚úÖ Mirroring enabled for {self.database_name}\")\n","            else:\n","                print(\"üß© Non-Fabric runtime detected ‚Äî switching to simulation mode.\")\n","                print(\"‚ÑπÔ∏è Skipping ALTER DATABASE (Fabric-only feature).\")\n","        except Exception as e:\n","            print(f\"‚ö†Ô∏è Mirroring note: {e}\")\n","            print(\"‚öôÔ∏è Continuing in simulation mode...\")\n","\n","    # ---------------------------------------------------------\n","    # Step 2 ‚Äì Ensure mock tables exist (create if missing)\n","    # ---------------------------------------------------------\n","    def _ensure_table_exists(self, table_name: str):\n","        full_name = f\"{self.database_name}.{table_name}\"\n","        try:\n","            self.spark.table(full_name)\n","            print(f\"‚úÖ Found source table: {full_name}\")\n","            return True\n","        except Exception:\n","            print(f\"‚ÑπÔ∏è Table not found: {full_name} ‚Äî creating mock data...\")\n","\n","            # Create mock DataFrame per table\n","            if table_name == \"ParkingSensorData\":\n","                df = self.spark.createDataFrame([\n","                    (\"Zone A\", True, \"2025-10-11 08:00\", True, \"tx001\"),\n","                    (\"Zone B\", False, \"2025-10-11 08:05\", True, \"tx002\"),\n","                ], [\"parking_zone\", \"occupancy_status\", \"timestamp\",\n","                    \"blockchain_verified\", \"blockchain_tx_id\"])\n","\n","            elif table_name == \"TrafficCameraData\":\n","                df = self.spark.createDataFrame([\n","                    (\"Cam-1\", \"HIGH\", 0.85, 34, \"2025-10-11 08:00\"),\n","                    (\"Cam-2\", \"MEDIUM\", 0.50, 20, \"2025-10-11 08:05\"),\n","                ], [\"camera_id\", \"congestion_level\", \"traffic_density\",\n","                    \"vehicle_count\", \"timestamp\"])\n","\n","            elif table_name == \"YOLOProcessedData\":\n","                df = self.spark.createDataFrame([\n","                    (\"Cam-1\", 0.92, \"SUCCESS\", \"2025-10-11 08:00\"),\n","                    (\"Cam-2\", 0.78, \"SUCCESS\", \"2025-10-11 08:05\"),\n","                ], [\"camera_id\", \"processing_confidence\",\n","                    \"processing_status\", \"processed_at\"])\n","            else:\n","                schema = StructType([StructField(\"id\", StringType())])\n","                df = self.spark.createDataFrame([], schema)\n","\n","            # Ensure database exists\n","            self.spark.sql(f\"CREATE DATABASE IF NOT EXISTS {self.database_name}\")\n","            df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(full_name)\n","            print(f\"‚úÖ Mock table created: {full_name}\")\n","            return True\n","\n","    # ---------------------------------------------------------\n","    # Step 3 ‚Äì Simulate Mirroring by copying to local schema\n","    # ---------------------------------------------------------\n","    def simulate_replication(self, table_names=None):\n","        if table_names is None:\n","            table_names = [\"ParkingSensorData\",\n","                           \"TrafficCameraData\",\n","                           \"YOLOProcessedData\"]\n","\n","        print(\"\\nüß± SIMULATING OPEN MIRRORING (Local Copy Mode)...\")\n","        self.spark.sql(f\"CREATE DATABASE IF NOT EXISTS {self.mirror_name}\")\n","\n","        for table in table_names:\n","            src_table = f\"{self.database_name}.{table}\"\n","            dst_table = f\"{self.mirror_name}.{table}\"\n","\n","            self._ensure_table_exists(table)\n","\n","            try:\n","                df = self.spark.table(src_table)\n","                df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(dst_table)\n","                print(f\"‚úÖ Mirrored table created: {dst_table} ({df.count()} rows)\")\n","            except Exception as e:\n","                print(f\"‚ö†Ô∏è Skipping {table}: {e}\")\n","\n","        print(\"üèÅ Mirroring simulation completed successfully.\\n\")\n","\n","    # ---------------------------------------------------------\n","    # Step 4 ‚Äì Export all tables (source + mirrored) safely\n","    # ---------------------------------------------------------\n","    def export_all_data(self):\n","        print(\"üì¶ Starting Safe Lakehouse Export (dbo-style)...\")\n","\n","        all_dbs = [self.database_name, self.mirror_name]\n","        for db in all_dbs:\n","            try:\n","                tables = [f\"{db}.{t.name}\" for t in self.spark.catalog.listTables(db)]\n","                for table in tables:\n","                    try:\n","                        print(f\"üì§ Exporting {table} ...\")\n","                        df = self.spark.table(table)\n","                        pdf = df.limit(100000).toPandas()\n","                        csv_path = os.path.join(self.output_dir, f\"{table.replace('.', '_')}.csv\")\n","                        pdf.to_csv(csv_path, index=False)\n","                        print(f\"‚úÖ Saved {csv_path} ({len(pdf)} rows)\")\n","                    except Exception as e:\n","                        print(f\"‚ö†Ô∏è Skipping {table}: {e}\")\n","            except Exception as e:\n","                print(f\"‚ö†Ô∏è Could not list tables in {db}: {e}\")\n","\n","        zip_path = f\"{self.output_dir}.zip\"\n","        shutil.make_archive(self.output_dir, \"zip\", self.output_dir)\n","        print(f\"üèÅ Export completed successfully ‚Üí {zip_path}\")\n","        return zip_path\n","\n","\n","# ---------------------------------------------------------\n","# ‚úÖ Optional standalone execution\n","# ---------------------------------------------------------\n","if __name__ == \"__main__\":\n","    spark = SparkSession.builder.appName(\"OpenMirroringWithExport\").getOrCreate()\n","    mgr = OpenMirroringManager(spark)\n","    mgr.configure_open_mirroring()\n","    mgr.simulate_replication()\n","    zip_path = mgr.export_all_data()\n","    print(f\"üì¶ All tables exported ‚Üí {zip_path}\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"e821c89f-0dfa-43fe-abbd-1632ed37e285","normalized_state":"finished","queued_time":"2025-10-22T03:57:50.0308158Z","session_start_time":"2025-10-22T03:57:50.0322504Z","execution_start_time":"2025-10-22T03:58:03.2331992Z","execution_finish_time":"2025-10-22T03:59:04.5640123Z","parent_msg_id":"1215872a-5156-4da6-9f8e-d787ec69a587"},"text/plain":"StatementMeta(, e821c89f-0dfa-43fe-abbd-1632ed37e285, 3, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["üöÄ STARTING OPEN MIRRORING SETUP\nüîÑ CONFIGURING OPEN MIRRORING FOR DELTA REPLICATION...\nüß© Non-Fabric runtime detected ‚Äî switching to simulation mode.\n‚ÑπÔ∏è Skipping ALTER DATABASE (Fabric-only feature).\n\nüß± SIMULATING OPEN MIRRORING (Local Copy Mode)...\n‚úÖ Found source table: ParkingDataLakehouse.ParkingSensorData\n‚úÖ Mirrored table created: MirroredData.ParkingSensorData (2 rows)\n‚úÖ Found source table: ParkingDataLakehouse.TrafficCameraData\n‚úÖ Mirrored table created: MirroredData.TrafficCameraData (2 rows)\n‚úÖ Found source table: ParkingDataLakehouse.YOLOProcessedData\n‚úÖ Mirrored table created: MirroredData.YOLOProcessedData (2 rows)\nüèÅ Mirroring simulation completed successfully.\n\nüì¶ Starting Safe Lakehouse Export (dbo-style)...\nüì§ Exporting ParkingDataLakehouse.parkingsensordata ...\n‚úÖ Saved /tmp/lakehouse_export_local/ParkingDataLakehouse_parkingsensordata.csv (2 rows)\nüì§ Exporting ParkingDataLakehouse.trafficcameradata ...\n‚úÖ Saved /tmp/lakehouse_export_local/ParkingDataLakehouse_trafficcameradata.csv (2 rows)\nüì§ Exporting ParkingDataLakehouse.yoloprocesseddata ...\n‚úÖ Saved /tmp/lakehouse_export_local/ParkingDataLakehouse_yoloprocesseddata.csv (2 rows)\nüì§ Exporting MirroredData.parkingsensordata ...\n‚úÖ Saved /tmp/lakehouse_export_local/MirroredData_parkingsensordata.csv (2 rows)\nüì§ Exporting MirroredData.trafficcameradata ...\n‚úÖ Saved /tmp/lakehouse_export_local/MirroredData_trafficcameradata.csv (2 rows)\nüì§ Exporting MirroredData.yoloprocesseddata ...\n‚úÖ Saved /tmp/lakehouse_export_local/MirroredData_yoloprocesseddata.csv (2 rows)\nüèÅ Export completed successfully ‚Üí /tmp/lakehouse_export_local.zip\nüì¶ All tables exported ‚Üí /tmp/lakehouse_export_local.zip\n"]}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6702b3b2-9bb3-4502-b07d-c98ab5ea0823"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"06b86e1a-668d-4d65-a5f0-7ba8a61fa57d"}],"default_lakehouse":"06b86e1a-668d-4d65-a5f0-7ba8a61fa57d","default_lakehouse_name":"ParkingDataLakehouse","default_lakehouse_workspace_id":"d3afc09a-dc29-418e-be57-836e9d2cc5f1"}}},"nbformat":4,"nbformat_minor":5}